# Copyright (C) 2015 Philipp Baumgaertel
# All rights reserved.
#
# This software may be modified and distributed under the terms
# of the BSD license.  See the LICENSE.txt file for details.


from math import fabs
import unittest
from skgpuppy.Covariance import GaussianCovariance, SPGPCovariance, Covariance, Dot, PeriodicCovariance
from skgpuppy.GaussianProcess import GaussianProcess
from skgpuppy.UncertaintyPropagation import UncertaintyPropagationNumerical, UncertaintyPropagationExact, UncertaintyPropagationLinear, UncertaintyPropagationApprox, UncertaintyPropagationMC, \
	UncertaintyPropagationNumericalHG
import numpy as np
import time
import pickle
import os
from skgpuppy.MLE import MLE
from skgpuppy.Utilities import norm, expected_value_monte_carlo
from skgpuppy.InverseUncertaintyPropagation import InverseUncertaintyPropagationApprox, InverseUncertaintyPropagationNumerical
from skgpuppy.PDF import Normal, Skew_Normal
from functools import wraps
import scipy
import sys

class TestPDF(unittest.TestCase):
	def test_skew_pdf(self):
		s = Skew_Normal()
		result = s.output_pdf(1,1,0.5,1,[1,2,3])

		#Numbers generated by R package sn
		self.assertAlmostEqual(result[0],0.398342403205,delta=1e-10 )
		self.assertAlmostEqual(result[1],0.202124583151,delta=1e-10 )
		self.assertAlmostEqual(result[2],0.0602633676246,delta=1e-10 )

		self.assertAlmostEqual(s.estimate_min_max(1,1,0.5,1,0.9)[0], -0.478543996503,delta=1e-5)
		self.assertAlmostEqual(s.estimate_min_max(1,1,0.5,1,0.9)[1], 2.79287447653,delta=1e-5)

		self.assertAlmostEqual(s.estimate_min_max(1,1,0.5,1,0.2)[0], 0.666406364782,delta=1e-5)
		self.assertAlmostEqual(s.estimate_min_max(1,1,0.5,1,0.2)[1], 1.16161779703,delta=1e-5)

	def test_norm_pdf(self):
		s = Skew_Normal()
		n = Normal()
		sresult = s.output_pdf(1,1,0,0,[1,2,3])
		nresult = n.output_pdf(1,1,0,0,[1,2,3])

		for i in range(3):
			self.assertAlmostEqual(sresult[i],nresult[i],delta=1e-10 )


		self.assertAlmostEqual(s.estimate_min_max(1,1,0,1,0.9)[0], n.estimate_min_max(1,1,0,1,0.9)[0],delta=1e-5)
		self.assertAlmostEqual(s.estimate_min_max(1,1,0,1,0.9)[1], n.estimate_min_max(1,1,0,1,0.9)[1],delta=1e-5)

		self.assertAlmostEqual(s.estimate_min_max(1,1,0,1,0.2)[0], n.estimate_min_max(1,1,0,1,0.2)[0],delta=1e-5)
		self.assertAlmostEqual(s.estimate_min_max(1,1,0,1,0.2)[1], n.estimate_min_max(1,1,0,1,0.2)[1],delta=1e-5)


class TestMLE(unittest.TestCase):
	def getCI(self, alpha, n, k):
		from scipy.stats import beta

		lowerci = beta.ppf(alpha / 2, k, n - k + 1)
		upperci = beta.ppf(1 - alpha / 2, k + 1, n - k)
		return lowerci, upperci

	def mle(self, n, k):
		lowerci, upperci = self.getCI(0.05, n, k)

		def density(x, theta):
			if 0.5 < x[0] <= 1:
				return (theta[0]) * 2 # *2 to be a density -> integrate .. = 1
			elif 0 <= x[0] <= 0.5:
				return (1 - theta[0]) * 2
			else:
				return 0

		samples = []
		for i in range(k):
			samples.append([1])

		for i in range(n - k):
			samples.append([0])

		p = float(k) / float(n)

		mle = MLE(density, [0.5],support=[(0,1)])

		theta = mle.mle(samples)
		sigma = mle.sigma(theta, observations=samples)
		lb, ub = mle.mle_ci(samples)
		fisher_sigma = mle.sigma(theta, n=len(samples))

		self.assertAlmostEqual(fisher_sigma[0], sigma[0], delta=1e-5)
		self.assertAlmostEqual(p, theta[0], delta=0.1 * p)
		self.assertAlmostEqual(lb[0], lowerci, delta=0.15 * lowerci)
		self.assertAlmostEqual(ub[0], upperci, delta=0.15 * upperci)




	def test_fisher(self):
		def density(x, theta):
			if 0.5 < x[0] <= 1:
				return (theta[0]) * 2 # *2 to be a density -> integrate .. = 1
			elif 0 <= x[0] <= 0.5:
				return (1 - theta[0]) * 2
			else:
				return 0

		mle = MLE(density,[0.5],support=[(0,1)])


		fisher = mle.get_fisher_function()
		fisher_2nd = mle.get_fisher_function(order=2)
		self.assertAlmostEqual(fisher([0.5],0),fisher_2nd([0.5],0),delta=1e-5)
		self.assertAlmostEqual(fisher([0.05],0),fisher_2nd([0.05],0),delta=1e-5)
		self.assertAlmostEqual(fisher([0.99],0),fisher_2nd([0.99],0),delta=1e-4)


		#
		#		theta = np.atleast_2d(np.linspace(1e-2,1-1e-2,100)).T
		#		#y = [fisher(theta_i,0) for theta_i in theta]
		#		y = [1/np.sqrt(fisher(theta_i,0)) for theta_i in theta]
		#		fig = plt.figure()
		#		plt.plot(theta,y)
		#		plt.title('Fisher Information')
		#		plt.show()
		#

#	@unittest.skip("Too slow")
#	def test_fisher_2d_norm(self):
#		density2d = lambda x, theta : norm(x[0],theta[0],theta[1])*norm(x[1],theta[2],theta[3])
#		#density2d = lambda x, theta : mvnorm(np.array(x),np.array(theta[0:2]),np.diag(theta[2:4]))
#		mle = MLE([(-4,4),(-4,4)], [0.5,0.5,1,1],density=density2d)
#		fisher = mle.get_fisher_function()
#
#		for i in xrange(4):
#			for j in xrange(4):
#				print i,", ",j
#				print fisher([0.5,0.5,1,1],i,j)


	def test_fisher_norm_matrix(self):
		density = lambda x, theta : norm(np.float128(x[0]),theta[0],theta[1])
		fisher_matrix = [[lambda theta : 1/theta[1]**2, lambda theta : 0],[lambda theta : 0,lambda theta : 2/theta[1]**2]]
		mle = MLE(density,[0,1],dims=1,fisher_matrix=fisher_matrix)
		fisher = mle.get_fisher_function(order=1)

		for i in range(2):
			for j in range(2):
				if i != j :
					self.assertAlmostEqual(fisher([0.5,1],i,j),0,delta=0.015)

		points = 100

		sigma = np.linspace(1e-1,0.2,points)
		theta = np.hstack((np.zeros((points,1)),np.atleast_2d(sigma).T))

		f_mu = [fisher(theta_i,0) for theta_i in theta]
		f_sigma = [fisher(theta_i,1) for theta_i in theta]

		f_mu2 = [1/x_i**2 for x_i in sigma]
		f_sigma2 = [2/(x_i**2) for x_i in sigma]

		for i in range(points):
			self.assertAlmostEqual(f_mu[i],f_mu2[i],delta=f_mu2[i]*1e-3)
			self.assertAlmostEqual(f_sigma[i],f_sigma2[i],delta=f_sigma2[i]*1e-3)



	def test_fisher_norm(self):
		density = lambda x, theta : norm(np.float128(x[0]),theta[0],theta[1])
		mle = MLE(density,[0,1],dims=1)#,support=[(-4,4)])
		fisher = mle.get_fisher_function(order=1)

		for i in range(2):
			for j in range(2):
#				print i,", ",j
#				print fisher([0,1],i,j)
				if i != j :
					self.assertAlmostEqual(fisher([0.5,1],i,j),0,delta=0.015)


		points = 100
		#mu = np.linspace(1e-1,2,points)
		#theta = np.array(np.hstack((np.atleast_2d(mu).T,np.zeros((points,1))+0.1)),dtype=np.float128)

		sigma = np.linspace(1e-1,0.2,points)
		theta = np.hstack((np.zeros((points,1)),np.atleast_2d(sigma).T))

		f_mu = [fisher(theta_i,0) for theta_i in theta]
		f_sigma = [fisher(theta_i,1) for theta_i in theta]
		#print type(f_mu[0])
		f_mu2 = [1/x_i**2 for x_i in sigma]
		f_sigma2 = [2/(x_i**2) for x_i in sigma]


		#y = [1/np.sqrt(fisher(theta_i,0)) for theta_i in theta]

#		import matplotlib.pyplot as plt
#
#		fig = plt.figure()
#		#plt.plot(mu,f_mu,'o')
#		#plt.plot(sigma,f_mu,'o',sigma,f_mu2)
#		#plt.plot(sigma,f_sigma,'o',sigma,f_sigma2)
#		plt.title('Fisher Information')
#		plt.show()

		for i in range(points):
			self.assertAlmostEqual(f_mu[i],f_mu2[i],delta=f_mu2[i]*1e-3)
			self.assertAlmostEqual(f_sigma[i],f_sigma2[i],delta=f_sigma2[i]*1e-3)

#
#		n = mle.get_n([0.1])
#		print n

	def test_fisher_2d(self):

		def density2d(x,theta):
			if 0 <= x[0] <= 1 and 0 <= x[1] <= 1:
				return theta[0]
			if 0 <= x[0] <= 1 and 1 <= x[1] <= 2:
				return 1-theta[0]
			if 1 <= x[0] <= 2 and 0 <= x[1] <= 1:
				return theta[1]
			if 1 <= x[0] <= 2 and 1 <= x[1] <= 2:
				return 1-theta[1]

		mle = MLE(density2d,[0.25,0.25],support=[(0,2),(0,2)])

		fisher = mle.get_fisher_function()
		fisher_2nd = mle.get_fisher_function(order=2)

		for i in range(2):

			self.assertAlmostEqual(fisher([0.25,0.25],i), fisher_2nd([0.25,0.25],i), delta=1e-3)
			for j in range(2):
				if i != j :
					self.assertAlmostEqual(fisher([0.25,0.25],i,j),0,1e-5)

	def test_mle(self):
		self.mle(400, 20)
		self.mle(400, 200)
		self.mle(4000, 200)
		self.mle(40000, 2000)




class TestInverseUP(unittest.TestCase):


	def setUp(self):
		np.random.seed(123456)

		# if not os.path.exists('tests/gp_2d.pkl'):
			#Because it takes too long: pickle a gp

		x = np.array([[x1,x2] for x1 in range(10) for x2 in range(10)],dtype=np.float64)  #np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04,0.04])
		v = 2
		vt = 0#.01

		theta = np.zeros(2+len(w))

		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)

		y = GaussianProcess.get_realisation(x, GaussianCovariance(), theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01



		self.gp_est = GaussianProcess(x, t, cov=GaussianCovariance())


		means, variances = self.gp_est.estimate_many(x)
		sigma = np.sqrt(variances)

		for i in range(len(x)):
			self.assertAlmostEqual(means[i], y[i], delta=4 * sigma[i])

		# 	output = open('tests/gp_2d.pkl', 'wb')
		#
		# 	pickle.dump(self.gp_est, output)
		# 	output.close()
		#
		# pkl_file = open('tests/gp_2d.pkl', 'rb')
		#
		# self.gp_est = pickle.load(pkl_file)
		# pkl_file.close()

	def test_GP_2D(self):
		inputmean = np.array([5.0,5.0])
		inputvariance = np.diag([0.2,0.3])

		upn = UncertaintyPropagationNumerical(self.gp_est)
		uph = UncertaintyPropagationNumericalHG(self.gp_est)
		#upn = UncertaintyPropagationMC(self.gp_est,100000)

		upe = UncertaintyPropagationExact(self.gp_est)

		upl = UncertaintyPropagationLinear(self.gp_est)

		upa = UncertaintyPropagationApprox(self.gp_est)

		start = time.time()
		meanL, varianceL = upl.propagate_GA(inputmean, inputvariance)
		end = time.time()
		print("Linear ", end - start)

		start = time.time()
		meanE, varianceE = upe.propagate_GA(inputmean, inputvariance)
		end = time.time()
		print("Exact ", end - start)

		start = time.time()
		meanN, varianceN = upn.propagate_GA(inputmean, inputvariance)
		end = time.time()
		print("Numerical ", end - start)

		start = time.time()
		meanH, varianceH = uph.propagate_GA(inputmean, inputvariance)
		end = time.time()
		print("Numerical HG", end - start)

		start = time.time()
		meanA, varianceA = upa.propagate_GA(inputmean, inputvariance)
		end = time.time()
		print("Approx ", end - start)


		print(meanH, meanN, meanE, meanL, meanA)
		print((varianceH),(varianceN), (varianceE), (varianceL), (varianceA))
		print(np.sqrt(varianceH),np.sqrt(varianceN), np.sqrt(varianceE), np.sqrt(varianceL), np.sqrt(varianceA))

		self.assertAlmostEqual(meanE, meanA, delta=0.002)
		self.assertAlmostEqual(meanH, meanE, delta=0.1)
		self.assertAlmostEqual(meanN, meanH, delta=0.01)


		self.assertAlmostEqual(np.sqrt(varianceE), np.sqrt(varianceA), delta=0.001)
		self.assertAlmostEqual(np.sqrt(varianceH), np.sqrt(varianceE), delta=0.05)
		self.assertAlmostEqual(np.sqrt(varianceN), np.sqrt(varianceH), delta=0.015)




	def test_IUPOpt(self):
		self.IUPOpt()

	def test_IUPOpt_wo_weaving(self):
		import skgpuppy.UncertaintyPropagation
		weaving = skgpuppy.UncertaintyPropagation.weaving
		skgpuppy.UncertaintyPropagation.weaving = False
		self.IUPOpt()
		skgpuppy.UncertaintyPropagation.weaving = weaving

	def IUPOpt(self):


		output_variances = 0.2
		u = np.array([5.0,5.0])
		gps = self.gp_est
		c = np.array([4.0,1.0])
		I = 1/c
		iupa = InverseUncertaintyPropagationApprox(output_variances,gps,u,c,I)
		iupn = InverseUncertaintyPropagationNumerical(output_variances,gps,u,c,I,upga_class=UncertaintyPropagationApprox)


		start = time.time()
		sol_n = iupn.get_best_solution()
		end = time.time()
		print("Numerical ", end - start)

		start = time.time()
		sol_a = iupa.get_best_solution()
		end = time.time()
		print("Approx ", end - start)

		up = UncertaintyPropagationExact(self.gp_est)


		print("Numerical, NA, Approx Solution")
		print(sol_n)

		print(sol_a)
		print("Numerical, NA, Approx Variances")
		print(up.propagate_GA(u,np.diag(sol_n))[1])

		print(up.propagate_GA(u,np.diag(sol_a))[1])
		print("Numerical, NA, Approx Costs")
		print(c[0] * 1/sol_n[0] + c[1] * 1/sol_n[1])

		print(c[0] * 1/sol_a[0] + c[1] * 1/sol_a[1])

		self.assertAlmostEqual(sol_n[0],sol_a[0],delta=1e-2)
		self.assertAlmostEqual(sol_n[1],sol_a[1],delta=1e-3)


class TestMinimize(unittest.TestCase):

	def test_minimize(self):
		from skgpuppy.Utilities import minimize
		func = lambda x: x[0]**2 + 2*x[1]**2 + 3* x[2]**2
		fprime = lambda x: np.array([2*x[0], 4*x[1], 6* x[2]])
		methods = ["tnc", "l_bfgs_b", "cobyla", "slsqp", "bfgs", "powell", "cg", "ncg", "simplex"]

		for method in methods:
			result = minimize(func,np.array([1.0,1.0,1.0]),fprime=fprime,method=method)
			self.assertAlmostEqual(result[0],0,delta=1e-4)
			self.assertAlmostEqual(result[1],0,delta=1e-4)
			self.assertAlmostEqual(result[2],0,delta=1e-4)

class TestIntegration(unittest.TestCase):

	def setUp(self):
		np.random.seed(1234)



	def test_integrate_nquad(self):
		from skgpuppy.Utilities import _integrate_nquad

		#Wolfram Alpha:
		#integrate x+y+z dx dy dz from x = 0 to 1 from y = 0 to 1 from z = 0 to 1
		self.assertAlmostEqual(_integrate_nquad(lambda x: x[0], [(0, 1)]), 0.5, delta=1e-5)
		self.assertAlmostEqual(_integrate_nquad(lambda x: x[0] + x[1], [(0, 1), (0, 1)]), 1, delta=1e-5)
		self.assertAlmostEqual(_integrate_nquad(lambda x: x[0] + x[1] + x[2], [(0, 1), (0, 1), (0, 1)]), 1.5, delta=1e-5)

		#Wolfram Alpha:
		#integrate x+2*y+3*z dx dy dz from x = 0 to 1 from y = 0 to 2 from z = 0 to 3
		#Maxima:
		#integrate(integrate(integrate(x+2*y+3*z,x,0,1),y,0,2),z,0,3);

		self.assertAlmostEqual(_integrate_nquad(lambda x: x[0] + 2 * x[1] + 3 * x[2], [(0, 1), (0, 2), (0, 3)]), 42,
							   delta=1e-5)

	def test_integrate_quad(self):
		from skgpuppy.Utilities import _integrate_quad

		#Wolfram Alpha:
		#integrate x+y+z dx dy dz from x = 0 to 1 from y = 0 to 1 from z = 0 to 1
		self.assertAlmostEqual(_integrate_quad(lambda x: x[0], [(0, 1)]), 0.5, delta=1e-5)
		self.assertAlmostEqual(_integrate_quad(lambda x: x[0] + x[1], [(0, 1), (0, 1)]), 1, delta=1e-5)
		self.assertAlmostEqual(_integrate_quad(lambda x: x[0] + x[1] + x[2], [(0, 1), (0, 1), (0, 1)]), 1.5, delta=1e-5)

		#Wolfram Alpha:
		#integrate x+2*y+3*z dx dy dz from x = 0 to 1 from y = 0 to 2 from z = 0 to 3
		#Maxima:
		#integrate(integrate(integrate(x+2*y+3*z,x,0,1),y,0,2),z,0,3);

		self.assertAlmostEqual(_integrate_quad(lambda x: x[0] + 2 * x[1] + 3 * x[2], [(0, 1), (0, 2), (0, 3)]), 42,
							   delta=1e-5)

	def test_integrate_romberg(self):
		from skgpuppy.Utilities import _integrate_romberg

		self.assertAlmostEqual(_integrate_romberg(lambda x: x[0], [(0, 1)]), 0.5, delta=1e-5)
		self.assertAlmostEqual(_integrate_romberg(lambda x: x[0] + x[1], [(0, 1), (0, 1)]), 1, delta=1e-5)
		self.assertAlmostEqual(_integrate_romberg(lambda x: x[0] + x[1] + x[2], [(0, 1), (0, 1), (0, 1)]), 1.5, delta=1e-5)

		#Wolfram Alpha:
		#integrate x+2*y+3*z dx dy dz from x = 0 to 1 from y = 0 to 2 from z = 0 to 3
		#Maxima:
		#integrate(integrate(integrate(x+2*y+3*z,x,0,1),y,0,2),z,0,3);

		self.assertAlmostEqual(_integrate_romberg(lambda x: x[0] + 2 * x[1] + 3 * x[2], [(0, 1), (0, 2), (0, 3)]), 42,
							   delta=1e-5)

	def test_expected_value_MC(self):
		mu = np.array([1,1])
		Sigma_x = np.array([[1,0],[0,1]])
		func = lambda x: x[0]*2

		self.assertAlmostEqual(2,expected_value_monte_carlo(func,mu,Sigma_x),delta=0.1)


class TestGaussianProcess(unittest.TestCase):
	def setUp(self):
		np.random.seed(1234)

	def test_gaussian_cov_derivative(self):
		x = np.atleast_2d(np.linspace(0, 10, 50)).T
		w = np.array([0.02])
		v = 2
		vt = 0.01

		n,d = np.shape(x)

		theta_min = np.ones(2+d)
		theta_min[0] = np.log(v)
		theta_min[1] = np.log(vt)
		theta_min[2:2+d] = np.log(w)

		n,d = np.shape(x)
		cov = GaussianCovariance()
		for xi in x:
			for xj in x:
				for j in range(2+d):
						self.assertAlmostEqual(Covariance._d_cov_d_theta(cov,xi,xj,theta_min,j),cov._d_cov_d_theta(xi,xj,theta_min,j),delta=1e-3)


	def test_spgp_covariance(self):

		spgpc = SPGPCovariance(10)
		gc = GaussianCovariance()
		x = np.array([[x1,x2] for x1 in range(10) for x2 in range(10)])  #np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04,0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))

		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)

		y = GaussianProcess.get_realisation(x, GaussianCovariance(),theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01

		theta = spgpc.get_theta(x,t)
		cov = spgpc.cov_matrix(x,theta)
		cov_super = Covariance.cov_matrix_ij(spgpc,x,x,theta)
		self.assertLessEqual(np.abs((cov-cov_super).sum()),1e-5)#-np.exp(theta[1])*np.eye(100)
		invcov = np.linalg.inv(cov)
		invcov2 = spgpc.inv_cov_matrix(x,theta)

		self.assertLessEqual(np.abs((invcov-invcov2)).sum(),1e-5)

		# theta_gc = gc.get_theta(x,t)
		# gcov = gc.cov_matrix(x,theta_gc)
		# invgcov = np.linalg.inv(gcov)
		# print np.max(np.abs(gcov-cov)/gcov)
		# print np.max(np.abs(invgcov-invcov)/invgcov)

		start = time.time()

		delta = 1e-5
		dndt_est = []
		for j in range(len(theta)):
			d = np.zeros(len(theta))
			d[j] = delta
			#dndt_est.append( (spgpc.negativeloglikelihood(x,t,theta+d) - spgpc.negativeloglikelihood(x,t,theta-d))/2/delta )
			dndt_est.append( (Covariance._negativeloglikelihood(spgpc,x,t,theta+d) - Covariance._negativeloglikelihood(spgpc,x,t,theta-d))/2/delta )

		print("TIME numerical: ",time.time() -start)

		# j = 0
		# d = np.zeros(len(theta))
		# d[j] = delta
		# d_cov_dt = (spgpc.cov_matrix(x,theta+d)-spgpc.cov_matrix(x,theta-d))/2/delta
		# print spgpc.d_cov_matrix_d_theta(x,theta,j) - d_cov_dt




		start = time.time()
		dndt = Covariance._d_nll_d_theta(spgpc,x,t,theta)
		print("TIME classic: ",time.time() -start)

		dot = Dot()
		dot.reset()
		start = time.time()
		dndt = spgpc._d_nll_d_theta(x,t,theta)
		print("TIME opt: ",time.time() -start)
		print(dot)

		self.assertLessEqual(np.abs((dndt_est-dndt)).sum(),1e-4)
		#self.assertLessEqual(np.abs((dndt_est-dndt_m)).sum(),1e-3)

		# gp_est_gc = GaussianProcess(x, t,GaussianCovariance())
		# gp_est_spgp = GaussianProcess(x, t,SPGPCovariance(5))
		#
		#
		# theta = gp_est_spgp.theta_min
		# cov = spgpc.cov_matrix(x,theta)
		#
		#
		# theta_gc = gp_est_gc.theta_min
		# gcov = gc.cov_matrix(x,theta_gc)
		# print np.max(np.abs(gcov-cov)/gcov)
		#
		# invcov = np.linalg.inv(cov)
		# invgcov = np.linalg.inv(gcov)
		# print np.max(np.abs(invgcov-invcov)/invgcov)
		#



	def test_covariance(self):
		gc = GaussianCovariance()

		x = np.array([[x1,x2] for x1 in range(10) for x2 in range(10)])  #np.atleast_2d(np.linspace(0, 10, 30)).T
		theta = np.log(np.array([2,0.01,0.04,0.04]))

		cov = gc.cov_matrix(x,theta)
		cov_super = Covariance.cov_matrix(gc,x,theta)
		self.assertLessEqual(np.abs((cov-cov_super-0.01*np.eye(100))).sum(),1e-10)

		dcov = gc._d_cov_matrix_d_theta(x,theta,2)
		dcov_super = Covariance._d_cov_matrix_d_theta(gc,x,theta,2)

		self.assertLessEqual(np.abs((dcov-dcov_super)).sum(),1e-10)

		#np.count_nonzero(cov-cov_super)
		t = GaussianProcess.get_realisation(x, GaussianCovariance(), theta)
		#t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01

		dndt = gc._d_nll_d_theta(x,t,theta)
		print(dndt)
		dndt_est = []
		for j in range(len(theta)):
			d = np.zeros(len(theta))
			d[j] = 1e-5
			#d = np.log(1+d/np.exp(theta)) # Addition of log: log(x+y) = log(x) + log(1+y/x)
			dndt_est.append( (gc._negativeloglikelihood(x,t,theta+d) - gc._negativeloglikelihood(x,t,theta-d))/2e-5 )


		dndt_est = np.array(dndt_est)
		print(dndt_est)
		print(np.abs(dndt_est - dndt))
		self.assertTrue((np.abs(dndt_est - dndt) < 5e-1).all())

	def test_pickle(self):
		x = np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))
		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)
		y = GaussianProcess.get_realisation(x, GaussianCovariance(), theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01

		gp_est = GaussianProcess(x, t, GaussianCovariance())
		output = open('gp.pkl', 'wb')

		pickle.dump(gp_est, output,protocol=0)
		output.close()

		pkl_file = open('gp.pkl', 'rb')
		if sys.version_info.major == 3:
			gp_unpickled = pickle.load(pkl_file, encoding='latin1')
		else:
			gp_unpickled = pickle.load(pkl_file)

		pkl_file.close()
		os.remove('gp.pkl')
		means, variances = gp_est.estimate_many(x)
		sigma = np.sqrt(variances)
		meansp, variancesp = gp_unpickled.estimate_many(x)
		sigmap = np.sqrt(variancesp)

		for i in range(len(x)):
			self.assertEqual(means[i], meansp[i])
			self.assertEqual(sigma[i], sigmap[i])

	def test_gp(self):
		x = np.atleast_2d(np.linspace(0, 10, 100)).T
		w = np.array([0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))
		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)

		y = GaussianProcess.get_realisation(x, GaussianCovariance(),theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01



		n = 10
		xt = np.atleast_2d(np.linspace(0, 10, 200)).T

		gp_est = GaussianProcess(x, t,GaussianCovariance())
		print("Theta:", np.exp(gp_est.theta_min[0:3]))
		means_gp, variances_gp = gp_est.estimate_many(xt)
		sigma_gp = np.sqrt(variances_gp)

		spgpcov = SPGPCovariance(n)
		gp_est = GaussianProcess(x, t,spgpcov)
		print("Theta:", np.exp(gp_est.theta_min[0:3]))
		means, variances = gp_est.estimate_many(xt)
		theta_start = spgpcov.get_theta(x,t)
		print("Thetastart:", np.exp(theta_start[0:3]))
		#means = spgpcov.estimate(x,t,gp_est.theta_min,xt)

		sigma = np.sqrt(variances)


		# import matplotlib.pyplot as plt
		# fig = plt.figure()
		# plt.plot(x,y,x,t,'o',xt,means,xt,means_gp)
		# plt.fill_between(xt.ravel(), means + 1.96 * sigma, means - 1.96 * sigma, facecolor='red',alpha=0.5)
		# plt.fill_between(xt.ravel(), means_gp + 1.96 * sigma_gp, means_gp - 1.96 * sigma_gp, facecolor='lightblue',alpha=0.5)
		# plt.title('own')
		# plt.legend(['Realisation','Noisy','Estimation','EstimationGP'])
		# plt.show()

		for i in range(len(x)):
			self.assertAlmostEqual(means_gp[i*2], y[i], delta=4 * sigma[i*2])
			self.assertAlmostEqual(means[i*2], y[i], delta=4 * sigma[i*2])

	def test_gp_2D(self):

		x = np.array([[x1,x2] for x1 in range(10) for x2 in range(10)])  #np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04,0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))

		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)
		y = GaussianProcess.get_realisation(x, GaussianCovariance(), theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01


		# gp_est = GaussianProcess(x, t)
		# x_new = np.array([[x1/2.0,x2/2.0] for x1 in xrange(20) for x2 in xrange(20)])
		# #x_new = x
		# means, variances = gp_est.estimate_many(x_new)
		# sigma = np.sqrt(variances)
		#
		# import pylab as p
		# # #import matplotlib.axes3d as p3
		# import mpl_toolkits.mplot3d.axes3d as p3
		#
		#
		#





		gp_est = GaussianProcess(x, t,SPGPCovariance(10))
		#gp_est = GaussianProcess(x, t,GaussianCovariance())

		means, variances = gp_est.estimate_many(x)
		sigma = np.sqrt(variances)

		for i in range(len(x)):
			self.assertAlmostEqual(means[i], y[i], delta=5 * sigma[i])


		# x_new = np.array([[x1/2.0,x2/2.0] for x1 in xrange(20) for x2 in xrange(20)])
		# #x_new = x
		# means, variances = gp_est.estimate_many(x_new)
		#
		# sigma = np.sqrt(variances)

		# import matplotlib.pyplot as plt
		# #
		# from mpl_toolkits.mplot3d import Axes3D
		# from matplotlib import cm
		# fig = plt.figure()
		# ax = fig.gca(projection='3d')
		# ax.plot_trisurf(x.T[0],x.T[1],y, cmap=cm.autumn, linewidth=0.2)
		# ax.plot_trisurf(x_new.T[0],x_new.T[1],means, cmap=cm.winter, linewidth=0.2)
		# # #ax.plot_trisurf(x, y, z-z_est, cmap=cm.jet, linewidth=0.2)
		# # #plt.zlim((-1,1))
		# plt.show()

	def test_spgp_nll(self):


		x = np.array([[x1,x2] for x1 in range(10) for x2 in range(10)])  #np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04,0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))

		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)
		y = GaussianProcess.get_realisation(x, GaussianCovariance(), theta)
		t = y + 0.1 * np.random.randn(len(x)) #-> vt = 0.01


		spgpcov = SPGPCovariance(10)
		gc = GaussianCovariance()
		theta = spgpcov.get_theta(x,t)
		theta_gc = gc.get_theta(x,t)

		start = time.time()
		res_g = gc._negativeloglikelihood(x,t,theta_gc)
		print("Gaussian NLL: ",res_g)
		print("Time: ",time.time()-start)

		start = time.time()
		res = Covariance._negativeloglikelihood(spgpcov,x,t,theta)
		print("My NLL: ",res)
		print("Time: ",time.time()-start)

		res_s =  spgpcov._negativeloglikelihood(x,t,theta)
		print("Snelson NLL: ", res_s)
		print("Time: ", time.time()-start)


		self.assertAlmostEqual(res,res_s,delta=2e-1)

class TestTaylor(unittest.TestCase):
	#TODO: test estimate many

	def test_Isserli(self):
		from skgpuppy.TaylorPropagation import _Isserli, _fast_isserli
		from itertools import product
		for order in [4,6,8]:

			n = 4
			c = 0
			c2 = 0
			c3 = 0
			Sigma_x = np.eye(n)
			for e in product(list(range(order+1)),repeat=n):
				if sum(e) == order:
					i = _Isserli(np.array(e),Sigma_x,diagonal=False)
					if i != 0:
						c += 1

					i = _Isserli(np.array(e),Sigma_x)
					if i != 0:
						c2 += 1

					i = _fast_isserli(np.array(e),Sigma_x)
					if i != 0:
						c3 += 1
			self.assertEqual(c,c2)
			self.assertEqual(c,c3)


		self.assertEqual(_Isserli(np.array([8,0,0,0]),Sigma_x),105)
		self.assertEqual(_Isserli(np.array([6,0,0,0]),Sigma_x),15)
		self.assertEqual(_Isserli(np.array([4,0,0,0]),Sigma_x),3)

		self.assertEqual(_fast_isserli(np.array([8,0,0,0]),Sigma_x),105)
		self.assertEqual(_fast_isserli(np.array([6,0,0,0]),Sigma_x),15)
		self.assertEqual(_fast_isserli(np.array([4,0,0,0]),Sigma_x),3)

	def test_derivative(self):
		from skgpuppy.TaylorPropagation import _ndderivative
		f = lambda x: np.sin(x[0]*x[1])
		d10 = lambda x: x[1] * np.cos(x[0] * x[1])
		d01 = lambda x: x[0] * np.cos(x[0] * x[1])
		d11 = lambda x: np.cos(x[0] * x[1]) - x[0] * x[1] * np.sin(x[0] * x[1])
		d20 = lambda x: -x[1]**2 * np.sin(x[0] * x[1])
		nddev = _ndderivative(f)
		self.assertAlmostEqual(nddev.ndderivative([1,1],[1,0]), d10([1,1]),delta=1e-5)
		self.assertAlmostEqual(nddev.ndderivative([1,1],[0,1]), d01([1,1]),delta=1e-5)
		self.assertAlmostEqual(nddev.ndderivative([1,1],[1,1]), d11([1,1]),delta=1e-4)
		self.assertAlmostEqual(nddev.ndderivative([1,1],[2,0]), d20([1,1]),delta=1e-5)


	def test_powerlists(self):
		from skgpuppy.TaylorPropagation import _get_powerlists
		from itertools import product
		import time

		order = 3
		dims = 6
		t = time.time()
		p_lists = []
		for e in product(list(range(order+1)),repeat=dims):
			# Very inefficient
			if sum(e) == order:
				p_lists.append(e)
		print(time.time()-t)

		t = time.time()
		p_lists_2 = _get_powerlists(order,dims)
		print(time.time()-t)

		self.assertEqual(len(p_lists),len(p_lists_2))


		for i,p in enumerate(p_lists):
			self.assertListEqual(list(p),list(p_lists_2[i]))


		#t = time.time()
		p_lists = []
		for e in product(list(range(order+1)),repeat=dims):
			# Very inefficient
			if sum(e) <= order:
				p_lists.append(e)
		#print time.time()-t

		#t = time.time()
		p_lists_2 = _get_powerlists(order,dims,leq=True)
		#print time.time()-t

		self.assertEqual(len(p_lists),len(p_lists_2))


		for i,p in enumerate(p_lists):
			self.assertListEqual(list(p),list(p_lists_2[i]))


	def test_Taylor_estimation(self):
		from skgpuppy.TaylorPropagation import TaylorPropagation
		from itertools import product
		f = lambda x: np.sin(x[0])

		#order 3 => dx = 1e-3
		#order 6 => dx = 1e-2
		#order 9 => dx = 1e-1

		t = TaylorPropagation(f,[1],10,dx=1e-1)

		self.assertAlmostEqual(t([1.1]),np.sin(1.1),delta=1e-3)
		#print t([1.1])
		#print np.sin(1.1)

		# import matplotlib.pyplot as plt
		# x_list = np.atleast_2d(np.linspace(-6, 6, 100)).T
		# y_list = np.sin(x_list)
		# y_est_list = t.estimate_many(x_list)
		# #print x_list
		# #print y_est_list
		# fig = plt.figure()
		# plt.ylim((-1,1))
		# plt.plot(x_list,y_list)
		# plt.plot(x_list,y_est_list)
		# plt.title('Taylor Series')
		# plt.show()

		f = lambda x: np.sin(x[0]*x[1])

		t = TaylorPropagation(f,[1,1],10,dx=1e-1)
		#print t([1.1,1.1])
		#print np.sin(1.1*1.1)
		self.assertAlmostEqual(t([1.1,1.1]),np.sin(1.1*1.1),delta=1e-3)

		# import matplotlib.pyplot as plt
		# xs = np.linspace(-0.5,2.5,30)
		# x_list = np.array(list(product(xs,repeat=2)))
		# x = x_list.T[0]
		# y = x_list.T[1]
		# z = np.sin(x*y)
		# z_est = np.array(t.estimate_many(x_list))
		#
		# from mpl_toolkits.mplot3d import Axes3D
		# from matplotlib import cm
		# fig = plt.figure()
		# ax = fig.gca(projection='3d')
		# ax.plot_trisurf(x, y, z, cmap=cm.jet, linewidth=0.2)
		# ax.plot_trisurf(x, y, z_est, cmap=cm.jet, linewidth=0.2)
		# #ax.plot_trisurf(x, y, z-z_est, cmap=cm.jet, linewidth=0.2)
		# #plt.zlim((-1,1))
		# plt.show()


	def test_taylor_propagation(self):

		from skgpuppy.TaylorPropagation import TaylorPropagation
		f = lambda x: np.sin(x[0]*x[1])
		stddev = 0.4
		Sigma_x = np.diag(np.array([stddev,stddev])**2)

		start = time.time()
		vals = np.random.multivariate_normal(np.array([1,1]),Sigma_x,10**7).T
		out = f(vals)
		print(time.time()-start)

		print(out.mean(), out.std())
		#mean =  t.propagate_mean(Sigma_x)

		start = time.time()
		t = TaylorPropagation(f,[1,1],10,dx=1e-1)
		mean,variance =  t.propagate(Sigma_x)
		print(time.time()-start)
		print(mean, np.sqrt(variance))
		self.assertAlmostEqual(out.mean(),mean,delta=1e-3)
		self.assertAlmostEqual(out.std(),np.sqrt(variance),delta=1e-3)


	def test_hermgauss(self):
		from numpy.polynomial.hermite import hermgauss
		from scipy.integrate import quad

		x,w = hermgauss(100)
		y = []
		sqrt2 = np.sqrt(2)
		for xi in x:
			y.append(np.cos(xi))
		y = np.array(y)
		h = lambda x: np.cos(x)*np.exp(-x**2)
		self.assertAlmostEqual( (y*w).sum(), quad(h,-10,10)[0], delta=1e-5)

		from skgpuppy.FFNI import FullFactorialNumericalIntegrationNaive
		from skgpuppy.Utilities import integrate_hermgauss_nd, integrate_hermgauss
		f = lambda x: np.sin(x[0])
		ffni = FullFactorialNumericalIntegrationNaive(f,np.array([1]))
		self.assertAlmostEqual(integrate_hermgauss(f,1,2,order=100),ffni.propagate(np.array([[4]]))[0],delta=1e-4)

		f = lambda x: np.cos(x[0]*x[1])
		stddev = 0.3
		Sigma_x = np.diag(np.array([stddev,stddev])**2)
		ffni = FullFactorialNumericalIntegrationNaive(f,np.array([1.0,1.0]))

		self.assertAlmostEqual(integrate_hermgauss_nd(f,[1.0,1.0],Sigma_x,10),ffni.propagate(Sigma_x)[0],delta=1e-4)


	def test_skewness(self):
		from skgpuppy.TaylorPropagation import TaylorPropagation
		from skgpuppy.FFNI import FullFactorialNumericalIntegrationEvans, FullFactorialNumericalIntegrationHermGauss
		from scipy.stats import norm, scoreatpercentile

		f = lambda x: np.sin(x[0]*x[1])#*x[2]*x[3]*x[4]*x[5]*x[6]*x[7]
		stddev = 0.3
		Sigma_x = np.diag(np.array([stddev,stddev])**2)#,stddev,stddev
		mu = [1.0,1.0]#,1.0,1.0
		start = time.time()
		vals = np.random.multivariate_normal(np.array(mu),Sigma_x,10**7).T
		out = f(vals)
		print(time.time()-start)


		print("Monte Carlo")
		from scipy.stats import skew, kurtosis
		skewness_mc = skew(out)
		kurtosis_mc = kurtosis(out,fisher=False)
		print(out.mean(), out.std(), skewness_mc, kurtosis_mc)#, skewness_mc2, kurtosis_mc2

		print()
		print("Taylor")
		start = time.time()
		t = TaylorPropagation(f,mu,5,dx=1e-1)
		mean,variance,skewness,kurtosis =  t.propagate(Sigma_x,skew=True,kurtosis=True)
		print(time.time()-start)
		print(mean, np.sqrt(variance),skewness, kurtosis)


		self.assertAlmostEqual(out.mean(),mean,delta=0.002)
		self.assertAlmostEqual(out.std(),np.sqrt(variance),delta=0.002)
		self.assertAlmostEqual(skewness,skewness_mc,delta=1e-1)
		self.assertAlmostEqual(kurtosis,kurtosis_mc,delta=0.5)

		print()
		print("Evans")
		start = time.time()
		t = FullFactorialNumericalIntegrationEvans(f,np.array(mu))
		mean,variance,skewness,kurtosis =  t.propagate(Sigma_x,skew=True,kurtosis=True)
		print(time.time()-start)
		print(mean, np.sqrt(variance),skewness, kurtosis)

		#Evans is somehow awful
		self.assertAlmostEqual(out.mean(),mean,delta=0.002)
		self.assertAlmostEqual(out.std(),np.sqrt(variance),delta=0.002)
		#self.assertAlmostEqual(skewness,skewness_mc,delta=1e-1)
		#self.assertAlmostEqual(kurtosis,kurtosis_mc,delta=0.5)

		print()
		print("Gauss-Hermite")
		start = time.time()
		t = FullFactorialNumericalIntegrationHermGauss(f,np.array(mu),order=5)
		mean,variance,skewness,kurtosis =  t.propagate(Sigma_x,skew=True,kurtosis=True)
		print(time.time()-start)
		print(mean, np.sqrt(variance),skewness, kurtosis)

		self.assertAlmostEqual(out.mean(),mean,delta=0.002)
		self.assertAlmostEqual(out.std(),np.sqrt(variance),delta=0.002)
		self.assertAlmostEqual(skewness,skewness_mc,delta=1e-1)
		self.assertAlmostEqual(kurtosis,kurtosis_mc,delta=0.5)

		# print
		# print "Genz-Keister"
		# start = time.time()
		# t = FullFactorialNumericalIntegrationGenzKeister(f,np.array(mu),order=5)
		# mean,variance,skewness,kurtosis =  t.propagate(Sigma_x,skew=True,kurtosis=True)
		# print time.time()-start
		# print mean, np.sqrt(variance),skewness, kurtosis

		#
		# self.assertAlmostEqual(out.mean(),mean,delta=0.002)
		# self.assertAlmostEqual(out.std(),np.sqrt(variance),delta=0.002)
		# self.assertAlmostEqual(skewness,skewness_mc,delta=1e-1)
		# self.assertAlmostEqual(kurtosis,kurtosis_mc,delta=0.5)


		# n = Normal()
		# _min,_max = n.estimate_min_max(mean,variance,skewness,kurtosis,0.95)
		# print "Normal:"
		# print _min, _max
		#
		# s = Skew_Normal()
		# _min,_max = s.estimate_min_max(mean,variance,skewness,kurtosis,0.95)
		# print "Skew Normal:"
		# print _min, _max

		# p = Pearson()
		# _min,_max = p.estimate_min_max(mean,variance,skewness,kurtosis,0.95)
		# print "Pearson"
		# print _min, _max
		# _min_sample = scoreatpercentile(out,2.5)
		# _max_sample = scoreatpercentile(out,97.5)
		# print "MC"
		# print _min_sample,_max_sample
		# self.assertAlmostEqual(_min,_min_sample,delta=2e-2)
		# self.assertAlmostEqual(_max,_max_sample,delta=2e-2)
		#
		#plotting
		
		# import matplotlib.pyplot as plt
		# x_list = np.linspace(-1, 1.5, 100)
		# # y_list = p.output_pdf(mean,variance,skewness,kurtosis,x_list)
		# y_list_sn = s.output_pdf(mean,variance,skewness,kurtosis,x_list)
		# y_list_n = n.output_pdf(mean,variance,skewness,kurtosis,x_list)
		# #print x_list
		# #print y_est_list
		# fig = plt.figure()
		# #plt.ylim((-1,1))
		# # plt.plot(x_list,y_list, label="Pearson")
		# plt.plot(x_list,y_list_n,label="Normal")
		# plt.plot(x_list,y_list_sn,label="Skew Normal")
		# plt.hist(out,bins=100,histtype='step',normed=True)
		# plt.legend(['Pearson','Normal','Skew Normal', 'Hist'])
		#
		# plt.title('Output PDF')
		# plt.show()





class TestUncertaintyPropagationSPGP(unittest.TestCase):
	def setUp(self):
		np.random.seed(1234)

		self.x = np.atleast_2d(np.linspace(0, 10, 30)).T
		w = np.array([0.04])
		v = 2
		vt = 0#.01
		theta = np.zeros(2+len(w))

		theta[0] = np.log(v)
		theta[1] = np.log(vt)
		theta[2:2+len(w)] = np.log(w)
		y = GaussianProcess.get_realisation(self.x, GaussianCovariance(),theta)
		self.t = y + 0.1 * np.random.randn(len(self.x)) #-> vt = 0.01

		self.gp_est = GaussianProcess(self.x, self.t, cov=GaussianCovariance())

#		means, variances = self.gp_est.estimate_many(self.x)
#		sigma = np.sqrt(variances)
#
#		import matplotlib.pyplot as plt
#
#		fig = plt.figure()
#		plt.plot(x,y,x,t,'o')
#		plt.fill_between(x.ravel(), means + 1.96 * sigma, means - 1.96 * sigma, facecolor='lightblue')
#		plt.title('own')
#		plt.legend(['Realisation','Noisy','Estimation'])
#		#plt.draw()
#		plt.show()

	def test_propagate(self):
		self.propagate(5.0, 1e-3)
		self.propagate(3.0, 1e-3)
		self.propagate(8.0, 1e-3)

	def propagate(self,inputmean,inputvariance):
		upmc = UncertaintyPropagationMC(self.gp_est,10000)
		upn = UncertaintyPropagationNumerical(self.gp_est)
		uph = UncertaintyPropagationNumericalHG(self.gp_est)

		mean, var = uph.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		print(mean, var)

		#Try some values in range [mean,mean+2*std]
		for i in np.linspace(mean,mean+2*np.sqrt(var),5):
			start = time.time()
			t_n = upn.propagate(i,np.array([inputmean]), np.array([[inputvariance]]))
			end = time.time()

			start = time.time()
			t_mc = upmc.propagate(i,np.array([inputmean]), np.array([[inputvariance]]))
			end = time.time()

			start = time.time()
			t_h = uph.propagate(i,np.array([inputmean]), np.array([[inputvariance]]))
			end = time.time()


			t_ga =  scipy.stats.norm.pdf(i, loc=mean, scale=np.sqrt(var))

			print(t_n)
			print(t_mc)
			print(t_h)
			print(t_ga)
			#Fails
			#self.assertAlmostEqual(t_n, t_h, delta=t_h/10)
			self.assertAlmostEqual(t_h, t_mc, delta=t_h*1e-2)
			self.assertAlmostEqual(t_h, t_ga, delta=t_h*1e-4)

	def test_propagate_ga_approx(self):
		self.propagate_ga_approx(5.0, 0.3)
		self.propagate_ga_approx(3.0, 0.2)
		self.propagate_ga_approx(8.0, 0.1)


	def test_propagate_ga_approx_wo_weaving(self):
		import skgpuppy.UncertaintyPropagation
		weaving = skgpuppy.UncertaintyPropagation.weaving
		skgpuppy.UncertaintyPropagation.weaving = False
		self.propagate_ga_approx(5.0, 0.3)
		self.propagate_ga_approx(3.0, 0.2)
		self.propagate_ga_approx(8.0, 0.1)
		skgpuppy.UncertaintyPropagation.weaving = weaving

	def propagate_ga_approx(self, inputmean, inputvariance):
		upn = UncertaintyPropagationNumerical(self.gp_est)

		upe = UncertaintyPropagationExact(self.gp_est)

		upl = UncertaintyPropagationLinear(self.gp_est)

		upa = UncertaintyPropagationApprox(self.gp_est)


		start = time.time()
		meanE, varianceE = upe.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Exact ", end - start)


		start = time.time()
		meanA, varianceA = upa.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Approx ", end - start)


		print(meanE, meanA)
		print((varianceE), (varianceA))
		print(np.sqrt(varianceE), np.sqrt(varianceA))

		self.assertAlmostEqual(meanE, meanA, delta=1e-2)
		self.assertAlmostEqual(varianceE, varianceA, delta=1e-2)


	def test_propagate_ga(self):
		self.propagate_ga(5.0, 1.0)
		self.propagate_ga(3.0, 2.0)
		self.propagate_ga(8.0, 1.0)


	def propagate_ga(self, inputmean, inputvariance):
		upmc = UncertaintyPropagationMC(self.gp_est)

		upn = UncertaintyPropagationNumerical(self.gp_est)

		upe = UncertaintyPropagationExact(self.gp_est)

		upl = UncertaintyPropagationLinear(self.gp_est)

		upa = UncertaintyPropagationApprox(self.gp_est)

		uph = UncertaintyPropagationNumericalHG(self.gp_est)

		start = time.time()
		meanL, varianceL = upl.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Linear ", end - start)

		start = time.time()
		meanE, varianceE = upe.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Exact ", end - start)

		start = time.time()
		meanN, varianceN = upn.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Numerical ", end - start)

		start = time.time()
		meanH, varianceH = uph.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Numerical Herm Gauss", end - start)

		start = time.time()
		meanA, varianceA = upa.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("Approx ", end - start)

		start = time.time()
		meanMC, varianceMC = upmc.propagate_GA(np.array([inputmean]), np.array([[inputvariance]]))
		end = time.time()
		print("MC ", end - start)

		print(meanH, meanN, meanE, meanL, meanA, meanMC)
		print((varianceH), (varianceN), (varianceE), (varianceL), (varianceA), varianceMC)
		print(np.sqrt(varianceH),np.sqrt(varianceN), np.sqrt(varianceE), np.sqrt(varianceL), np.sqrt(varianceA), np.sqrt(varianceMC))

		self.assertAlmostEqual(meanN, meanH, delta=1e-4)
		self.assertAlmostEqual(meanN, meanL, delta=0.3)
		self.assertAlmostEqual(meanN, meanE, delta=1e-5)
		self.assertAlmostEqual(meanN, meanMC, delta=0.1)
		self.assertAlmostEqual(meanN, meanA, delta=0.1)
		self.assertAlmostEqual(varianceN, varianceH, delta=1e-3)
		self.assertAlmostEqual(varianceN, varianceE, delta=1e-5)
		self.assertAlmostEqual(varianceN, varianceA, delta=0.3)
		self.assertAlmostEqual(varianceN, varianceMC, delta=0.1)

		self.assertAlmostEqual(np.sqrt(varianceE), np.sqrt(varianceL), delta=np.sqrt(max(varianceL, varianceE)) * 1)

	def test_jacobian(self):
		jacobian = self.gp_est._get_Jacobian(np.array([5]),np.array([4]))

		est =  (self.gp_est._covariance(np.array([5]),np.array([4.1]))-self.gp_est._covariance(np.array([5]),np.array([3.9])))/0.2

		self.assertAlmostEqual(jacobian[0][0],est,delta=1e-2)

	def test_hessian(self):
		hessian = self.gp_est._get_Hessian(np.array([5]),np.array([4]))
		est = (self.gp_est._covariance(np.array([5]),np.array([4.1])) - 2 * self.gp_est._covariance(np.array([5]),np.array([4.0]))+self.gp_est._covariance(np.array([5]),np.array([3.9])))/0.01
		self.assertAlmostEqual(hessian[0][0],est,delta=1e-2)


class TestUncertaintyPropagationMETIS(unittest.TestCase):
	def setUp(self):

		# if not os.path.exists('tests/metis_gp.pkl'):
		# if os.path.exists('skgpuppy/tests/metis_data.pkl'):
		# 	with open('skgpuppy/tests/metis_data.pkl', 'rb') as output:
		# 		if sys.version_info.major == 3:
		# 			(collection,x,t) = pickle.load(output, encoding='latin1')
		# 		else:
		# 			(collection,x,t) = pickle.load(output)
		# else:
		# 	raise RuntimeError("Test data not found!")

		from skgpuppy.tests.metis_data import x,t

		self.gp_est = GaussianProcess(x, t,GaussianCovariance())
		# 	with open('tests/metis_gp.pkl', 'wb') as output:
		# 		pickle.dump(self.gp_est, output, protocol=-1)
		# else:
		# 	with open('tests/metis_gp.pkl', 'rb') as output:
		# 		self.gp_est = pickle.load(output)

		min = np.array([0.1,		0,		0])#,		200
		max = np.array([30, 		10,		0.05])#,		1000
		self.mean = (min+max)/2
		self.Sigma = np.diag([2**2,1**2,0.005**2])

	def test_propagation(self):
		#import skgpuppy.UncertaintyPropagation
		#skgpuppy.UncertaintyPropagation.weaving = False

		#from skgpuppy.UncertaintyPropagation2 import UncertaintyPropagationApprox, UncertaintyPropagationExact

		# Too large to ship the data for testing

		# if os.path.exists('skgpuppy/tests/metis_data_mc.pkl'):
		# 	with open('skgpuppy/tests/metis_data_mc.pkl', 'rb') as output:
		# 		if sys.version_info.major == 3:
		# 			(collection,x,t) = pickle.load(output, encoding='latin1')
		# 		else:
		# 			(collection,x,t) = pickle.load(output)
		# 	n = t.size
		# 	print(n)
		# 	print(t.mean(), t.std())
		# 	#With Fisher info sd:
		# 	ci_min = t.std()-1.96*t.std()/np.sqrt(n)/np.sqrt(2)
		# 	ci_max = t.std()+1.96*t.std()/np.sqrt(n)/np.sqrt(2)
		# 	print(ci_min, ci_max)
		#
		# 	# With Fisher info variance
		# 	self.assertAlmostEqual(np.sqrt(t.std()**2-1.96*np.sqrt(2)*t.std()**2/np.sqrt(n)),ci_min,delta=1e-5)
		# 	self.assertAlmostEqual(np.sqrt(t.std()**2+1.96*np.sqrt(2)*t.std()**2/np.sqrt(n)),ci_max,delta=1e-5)

			# with chi squared distribution:
			#print  np.sqrt((n-1)*t.std()**2 / chi2.ppf(1-0.05/2,n-1)), np.sqrt((n-1)*t.std()**2 / chi2.ppf(0.05/2,n-1))

		#else:

		ci_min = 0.0410788036621
		ci_max = 0.0422334526251
		meanG,varianceG = self.gp_est(self.mean)
		print("GP")
		print(meanG, np.sqrt(varianceG))
		print(self.gp_est._get_vt())
		code_u = varianceG - self.gp_est._get_vt()
		print("Code uncertainty: ", np.sqrt(code_u))
		self.assertLess(np.sqrt(code_u),0.0006)

		upe = UncertaintyPropagationExact(self.gp_est)
		start = time.time()
		meanE, varianceE = upe.propagate_GA(self.mean, self.Sigma)
		print("UPE")
		print(time.time()-start)
		print(meanE, np.sqrt(varianceE))
		print(np.sqrt(varianceE - code_u))
		self.assertLess(ci_min, np.sqrt(varianceE - code_u))
		self.assertLess(np.sqrt(varianceE - code_u),ci_max)

		upa = UncertaintyPropagationApprox(self.gp_est)
		start = time.time()
		meanA, varianceA = upa.propagate_GA(self.mean, self.Sigma)
		print("UPA")
		print(time.time() - start)
		print(meanA, np.sqrt(varianceA))
		print(np.sqrt(varianceA - code_u))
		self.assertLess(ci_min, np.sqrt(varianceA - code_u))
		self.assertLess(np.sqrt(varianceA - code_u),ci_max)


	def test_IUPOpt(self):

		meanG,varianceG = self.gp_est(self.mean)
		code_u = varianceG - self.gp_est._get_vt()

		output_variance =  0.005**2  + code_u #0.005**2
		u = self.mean
		gps = self.gp_est
		c = np.ones(3) # cost/fisher and fisher = 1/rate**2
		I = np.array([1/u[1]**2,2/(u[1]**2),1/(u[2]*(1-u[2]))])
		iupa = InverseUncertaintyPropagationApprox(output_variance,gps,u,c,I,coestimated=[[0,1]])
		iupn = InverseUncertaintyPropagationNumerical(output_variance,gps,u,c,I,upga_class=UncertaintyPropagationApprox,coestimated=[[0,1]])
		iupne = InverseUncertaintyPropagationNumerical(output_variance,gps,u,c,I,upga_class=UncertaintyPropagationExact,coestimated=[[0,1]])
		upa = UncertaintyPropagationApprox(self.gp_est)
		upe = UncertaintyPropagationExact(self.gp_est)


		#file = open("output/IUPOpt_out_var_2.txt",'a')

		start = time.time()
		sol_a = iupa.get_best_solution()
		end = time.time()
		print("Approx ", end - start)
		print(c/I/sol_a)
		#Testing coestimated
		self.assertAlmostEqual((c/I/sol_a)[0],(c/I/sol_a)[1],delta=1e-5)
		print(np.sqrt(upa.propagate_GA(u,np.diag(sol_a))[1]-code_u))
		print(np.sqrt(upe.propagate_GA(u,np.diag(sol_a))[1]-code_u))
		self.assertAlmostEqual(np.sqrt(upa.propagate_GA(u,np.diag(sol_a))[1]-code_u),np.sqrt(upe.propagate_GA(u,np.diag(sol_a))[1]-code_u),delta=1e-5)
		a_costs =  (np.array([1,0,1])*c/I * 1.0/np.array(sol_a)).sum()
		print(a_costs)



		start = time.time()
		sol_n = iupn.get_best_solution()
		end = time.time()
		print("Numerical ", end - start)
		print(c/I/sol_n)
		self.assertAlmostEqual((c/I/sol_n)[0],(c/I/sol_n)[1],delta=1e-5)
		print(np.sqrt(upa.propagate_GA(u,np.diag(sol_n))[1]-code_u))
		print(np.sqrt(upe.propagate_GA(u,np.diag(sol_n))[1]-code_u))
		self.assertAlmostEqual(np.sqrt(upa.propagate_GA(u,np.diag(sol_n))[1]-code_u),np.sqrt(upe.propagate_GA(u,np.diag(sol_n))[1]-code_u),delta=1e-5)
		n_costs =  (np.array([1,0,1])*c/I * 1.0/np.array(sol_n)).sum()
		print(n_costs)


		start = time.time()
		sol_ne = iupne.get_best_solution()
		end = time.time()
		print("Numerical Exact", end - start)
		print(c/I/sol_ne)
		self.assertAlmostEqual((c/I/sol_ne)[0],(c/I/sol_ne)[1],delta=1e-5)
		print(np.sqrt(upa.propagate_GA(u,np.diag(sol_ne))[1]-code_u))
		print(np.sqrt(upe.propagate_GA(u,np.diag(sol_ne))[1]-code_u))
		self.assertAlmostEqual(np.sqrt(upa.propagate_GA(u,np.diag(sol_ne))[1]-code_u),np.sqrt(upe.propagate_GA(u,np.diag(sol_ne))[1]-code_u),delta=1e-5)
		ne_costs =  (np.array([1,0,1])*c/I * 1.0/np.array(sol_ne)).sum()
		print(ne_costs)


		self.assertAlmostEqual(a_costs,n_costs,delta=1e-1)
		self.assertAlmostEqual(a_costs,ne_costs,delta=5e2)



#TODO: Use the MM1 sim as a Testcase


class TestPeriodicCovariance(unittest.TestCase):
	def setUp(self):
		np.random.seed(1234)
		self.x = np.atleast_2d(np.linspace(0, 10, 50)).T
		w = np.array([0.02])
		w2 = np.array([2])
		v = 2
		vt = 0.01
		p = np.array([3])
 
		# v = np.exp(theta[0])
		# vt = np.exp(theta[1])
		# w = np.exp(theta[2:2+d])
		# p = np.exp(theta[2+d:2+2*d])
		# w2 = np.exp(theta[2+2*d:])
		n,d = np.shape(self.x)
		self.assertEqual(d,1)
		
		self.theta_min = np.ones(2+3*d)
		self.theta_min[0] = np.log(v) 
		self.theta_min[1] = np.log(vt)
		self.theta_min[2:2+d] = np.log(w)
		self.theta_min[2+d:2+2*d] = np.log(p)
		self.theta_min[2+2*d:2+3*d] = np.log(w2)

		self.y = GaussianProcess.get_realisation(self.x, PeriodicCovariance(),self.theta_min)

	def test_cov_derivative(self):
		n,d = np.shape(self.x)
		cov = PeriodicCovariance()
		for xi in self.x:
			for xj in self.x:
				for j in range(2+3*d):
						self.assertAlmostEqual(Covariance._d_cov_d_theta(cov,xi,xj,self.theta_min,j),cov._d_cov_d_theta(xi,xj,self.theta_min,j),delta=1e-3)

	def test_cov(self):
		print("Theta_min: ", self.theta_min)
		x = np.atleast_2d(np.linspace(0.1, 9.9, 200)).T # upper bound 15 to show prediction
		gp_est = GaussianProcess(self.x, self.y,PeriodicCovariance())
		means, variances = gp_est.estimate_many(x)
		sigma = np.sqrt(variances)

		gp_est2 = GaussianProcess(self.x, self.y,PeriodicCovariance(),self.theta_min)
		means2, variances2 = gp_est2.estimate_many(x)
		sigma2 = np.sqrt(variances2)

		for i,mean in enumerate(means):
			self.assertAlmostEqual(mean,means2[i],delta=5e-2)
			self.assertAlmostEqual(sigma[i],sigma2[i],delta=5e-2)

		# import matplotlib.pyplot as plt
		# fig = plt.figure()
		# plt.plot(self.x,self.y,'o',x,means2)#,x,means
		#
		# #plt.fill_between(x.ravel(), means + 1.96 * sigma, means - 1.96 * sigma, facecolor='lightblue')
		# plt.fill_between(x.ravel(), means2 + 1.96 * sigma2, means2 - 1.96 * sigma2, facecolor='red')
		# plt.title('periodic')
		# plt.legend(['Realisation','GP'])
		# #plt.draw()
		# plt.show()


		# for i,xi in enumerate(self.x):
		# 	self.assertLess(means[i]-3*sigma[i],self.y[i])
		# 	self.assertLess(self.y[i],means[i]+3*sigma[i])
