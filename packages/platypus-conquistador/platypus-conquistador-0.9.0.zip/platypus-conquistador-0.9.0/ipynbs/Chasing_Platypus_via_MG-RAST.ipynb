{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "Copyright (c) 2015--, platypus development team.\n",
    "\n",
    "Distributed under the terms of the BSD License.\n",
    "\n",
    "The full license is in the file [COPYING.txt](https://raw.githubusercontent.com/biocore/Platypus-Conquistador/master/COPYING.txt), distributed with this software.\n",
    "\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chasing the Platypus via MG-RAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers downloading all public MG-RAST datasets; parsing those files to search for a specific taxa of interest, in our case platypus (Ornithorhynchus), Salmonella, dodo bird (Raphus) and Tasmanian tiger( Thylacinus); and finding the geographycal location of the samples that match those samples, via Google. Note that you need [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/#Download/) installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading all public MG-RAST datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you continue:** This step is extremelly slow (~24 hrs.) and network intensive. Thus to facilitate this step you could download the files from April 22, 2015 [ftp://ftp.microbio.me/pub/platypus/mgrast_reports_042215.tgz](ftp://ftp.microbio.me/pub/platypus/mgrast_reports_042215.tgz). If you place them in the same folder that you have this IPython Notebook the code will simply download the latests samples.\n",
    "\n",
    "The first step is to crawl MG-RAST's website to search for all public datasets, downloading both the data (in HTML format) and the metadata (tab separated) for each study and store them locally. To avoid having hundreds of files in a single directory the script will group them in folders by the first 3 characters of its name. Note that MG-RAST calls each sample a project, here we will call them samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Downloading previous version\n",
    "!curl -O ftp://ftp.microbio.me/pub/platypus/mgrast_reports_042215.tgz\n",
    "!tar zxvf mgrast_reports_042215.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from urllib2 import urlopen, URLError\n",
    "from os.path import isfile, exists, join\n",
    "from os import makedirs\n",
    "from socket import timeout as TOError\n",
    "\n",
    "mgrast_analysis = 'http://metagenomics.anl.gov/metagenomics.cgi?page=Analysis'\n",
    "mgrast_metagenome = ('http://metagenomics.anl.gov/metagenomics.cgi?page='\n",
    "                     'MetagenomeOverview&metagenome=')\n",
    "mgrast_metadata = ('http://metagenomics.anl.gov/metagenomics.cgi?page='\n",
    "                   'MetagenomeProject&action=download_md&filetype=text&'\n",
    "                   'filename=mgm%s.metadata.txt')\n",
    "\n",
    "page = urlopen(mgrast_analysis)\n",
    "soup = BeautifulSoup(page.read())\n",
    "count = 0\n",
    "all_inputs = soup.findAll('input', {'id': 'list_select_data_0'})[0]\n",
    "print '====> Got Analysis page results'\n",
    "for maingroups in all_inputs['value'].encode('utf-8').split('||'):\n",
    "    for mg in maingroups.split('@'):\n",
    "        sample = mg.split('##')[0].split('~#')[0]\n",
    "        # will group by the first 3 digits of the project name\n",
    "        dirname = sample[:3]\n",
    "\n",
    "        sample_fp = join(dirname, 'sample_' + sample + '.txt')\n",
    "        map_fp = join(dirname, 'map_' + sample + '.txt')\n",
    "        \n",
    "        count = count + 1\n",
    "        if count % 50 == 0:\n",
    "            print \"====> Processed %d samples\" % count\n",
    "        # continue if sample is blank or file already exists\n",
    "        if sample == '' or (isfile(sample_fp) and isfile(map_fp)):\n",
    "            continue\n",
    "\n",
    "        print 'Processing (%d): %s %s' % (count, sample, mg)\n",
    "\n",
    "        if not exists(dirname):\n",
    "            makedirs(dirname)\n",
    "\n",
    "        # visiting the Overview page to download counts\n",
    "        try:\n",
    "            page_sample = urlopen(mgrast_metagenome + sample, timeout=40)\n",
    "            page_mapping = urlopen(mgrast_metadata % sample, timeout=40)\n",
    "        except (TOError, URLError), e:\n",
    "            # Timeout\n",
    "            print e\n",
    "            count = count - 1\n",
    "            continue\n",
    "        \n",
    "        open(sample_fp, 'w').write(page_sample.read())\n",
    "        open(map_fp, 'w').write(page_mapping.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you continue:** This step is not as slow as the previous one but still a bit slow (~1 hr). If you only want to reproduce the results from the paper you can download this file: [ftp://ftp.microbio.me/pub/platypus/mgrast_search_results.tgz](ftp://ftp.microbio.me/pub/platypus/mgrast_search_results.tgz)\n",
    "\n",
    "Now that we have all MG-RAST files in our local folder we can search for any taxa of interest. Note that the HMTL filess that we downloaded in the previous step have all data information that MG-RAST provides from a sample, for example: multilevel taxonomic and functional assignments. Thus, in the next step you can search by any \"string\" that fits in those categories. The output of this section is tab separated text file named as the query (lower case) with three columns: the name found, the sample name and counts of the successful search on that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading previous version\n",
    "!curl -O ftp://ftp.microbio.me/pub/platypus/mgrast_search_results.tgz\n",
    "!tar zxvf mgrast_search_results.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir, isfile, join, basename, exists\n",
    "\n",
    "queries = ['Ornithorhynchus', \n",
    "           'Salmonella', \n",
    "           'Raphus', \n",
    "           'Thylacinus']\n",
    "\n",
    "queries = [q.lower() for q in queries]\n",
    "\n",
    "queries_fp = [open(q + '.txt', 'w') for q in queries]\n",
    "\n",
    "# adding a \" at the beginning of the queries so we do exact matching\n",
    "queries = ['\"%s' % q for q in queries]\n",
    "\n",
    "# looping in local dir to find downloaded folders\n",
    "# all mgrast created folders are 3 chars long and start with a 4\n",
    "dirs = [d for d in listdir('./')\n",
    "        if len(d) == 3 and d.startswith('4') and isdir(d)]\n",
    "\n",
    "count_files = 0\n",
    "for d in dirs:\n",
    "    # getting files in current directory\n",
    "    files = [join(d, f) for f in listdir(d) if f.startswith('sample_')]\n",
    "    for sample in files:\n",
    "        count_files = count_files + 1\n",
    "        if count_files % 250 == 0:\n",
    "            print 'Processed %d files' % count_files\n",
    "        sample_id = basename(f)[len('sample_'):-len('.txt')]\n",
    "        mapping = join(d, 'map_%s.txt' % sample_id)\n",
    "\n",
    "        # validating that we have both the sample and the metadata file for\n",
    "        # the given project\n",
    "        if not isfile(sample) or not isfile(mapping):\n",
    "            raise ValueError('Missing files %s, %s' % (sample, mapping))\n",
    "\n",
    "        soup = BeautifulSoup(open(sample, 'U').read())\n",
    "        for s in soup.findAll('script'):\n",
    "            s = str(s).lower()\n",
    "            \n",
    "            for q, qfp in zip(queries, queries_fp):\n",
    "                if q in s:\n",
    "                    target = [t for t in s.split('\\n') if q in t][0]\n",
    "                    name, counts = target.split(',')\n",
    "                    name = name.split('data.addrow([')[1].strip('\"')\n",
    "                    counts = counts[:counts.find(']')]\n",
    "                    qfp.write('%s\\t%s\\t%s\\n' % (name, sample_id, counts))\n",
    "                \n",
    "_ = [q.close() for q in queries_fp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We suggest reviewing the resulting files, in specific check that the first column actually matches your query. Depending on the how broad the term is, it could have hits to other taxa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding geographical location of samples and summarizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to generate a dictionary of dictonaries containing the counts per country of all the files generated in the previous step. Note that you need to feed which files you want to parse. To find the country of origin of the samples we will use the latitute and longitude values from the metadata. The speed on this step depends a lot on the number of hits and the diversity of countries in the results. For the example it took a little over 1 hr to run. Finally, remember that we are relying on an online (Google) service so the country summaries are based on its availability and successful reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "import json\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "from time import sleep\n",
    "\n",
    "           \n",
    "in_fps = ['ornithorhynchus.txt', \n",
    "          'raphus.txt', \n",
    "          'salmonella.txt', \n",
    "          'thylacinus.txt']\n",
    "\n",
    "# from http://stackoverflow.com/a/20169528/4228285\n",
    "def getplace(lat, lon):\n",
    "    url = (\"http://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "           \"latlng=%s,%s&sensor=false\" % (lat, lon))\n",
    "    # googleapis blocks requests that happen too fast so\n",
    "    # making sure to wait for 5 seconds before quering the\n",
    "    # api\n",
    "    sleep(5)\n",
    "    v = urlopen(url).read()\n",
    "    j = json.loads(v)\n",
    "\n",
    "    if len(j['results'])==0:\n",
    "        return 'Undefined'\n",
    "\n",
    "    components = j['results'][0]['address_components']\n",
    "    country = town = None\n",
    "    for c in components:\n",
    "        if \"country\" in c['types']:\n",
    "            country = c['long_name']\n",
    "\n",
    "    return str(country)\n",
    "\n",
    "print 'starting processing'\n",
    "sample_country = {}\n",
    "sample_country_counts = {fps:{} for fps in in_fps}\n",
    "for fps in in_fps:\n",
    "    print \"Processing file %s\" % fps\n",
    "    for line in open(fps, 'r'):\n",
    "        _, sample, counts = line.strip().split('\\t')\n",
    "        counts = int(counts)\n",
    "        \n",
    "        if sample not in sample_country:\n",
    "            mapping = join(sample[:3], 'map_%s.txt' % sample)\n",
    "\n",
    "            if not isfile(mapping):\n",
    "                raise ValueError(\"%s doesn't exist!!\" % (mapping))\n",
    "\n",
    "            # getting lat/long and quering Google for country\n",
    "            lat = ''\n",
    "            lon = ''\n",
    "            for line in open(mapping, 'r'):\n",
    "                line = line.split('\\t')\n",
    "\n",
    "                # sometimes the lat/lon have different comma separated values\n",
    "                # will only use the first one\n",
    "                if line[0] == 'Sample' and line[1] == 'longitude':\n",
    "                    lon = float(line[2].strip().split(',')[0].strip())\n",
    "                elif line[0] == 'Sample' and line[1] == 'latitude':\n",
    "                    lat = float(line[2].strip().split(',')[0].strip())\n",
    "\n",
    "                if not lat or not lon:\n",
    "                    country = 'Undefined'\n",
    "                else:\n",
    "                    country = getplace(lat, lon)\n",
    "            \n",
    "            sample_country[sample] = country\n",
    "        else:\n",
    "            country = sample_country[sample]\n",
    "        \n",
    "        if country not in sample_country_counts[fps]:\n",
    "            sample_country_counts[fps][country] = 0\n",
    "            \n",
    "        sample_country_counts[fps][country] += counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the results in a single file. The example files can be downloaded from [ftp://ftp.microbio.me/pub/platypus/mgrast_countries_results.tgz](ftp://ftp.microbio.me/pub/platypus/mgrast_countries_results.tgz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading previous version\n",
    "!curl -O ftp://ftp.microbio.me/pub/platypus/mgrast_countries_results.tgz\n",
    "!tar zxvf mgrast_countries_results.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "for fps, results in sample_country_counts.items():\n",
    "    sorted_results = sorted(results.items(),\n",
    "                            key=itemgetter(1)) \n",
    "    fps_out = open(fps[:-len('.txt')] + '_countries.txt', 'w')\n",
    "    for country, count in sorted_results:\n",
    "        fps_out.write(\"%s\\t%d\\n\" % (country, count))\n",
    "    fps_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use those results with http://jsfiddle.net/a1royfjh/ to generate some visualizations by copy/pasting the result of each of the next blocks in the bottom-left section. You need to replace the lines that are all the way to the left, which contains the country and counts information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fps = ['ornithorhynchus_countries.txt',\n",
    "       'raphus_countries.txt',\n",
    "       'salmonella_countries.txt',\n",
    "       'thylacinus_countries.txt']\n",
    "\n",
    "for fp in fps:\n",
    "    print '================================='\n",
    "    print 'Code for: %s' % fp\n",
    "    \n",
    "    for line in open(fp, 'r'):\n",
    "        country, counts = line.strip().split('\\t')\n",
    "        if country == 'Undefined':\n",
    "            print \"// ['%s', %s],\" % (country, counts)\n",
    "        else:\n",
    "            print \"['%s', %s],\" % (country, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
