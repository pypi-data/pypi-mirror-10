- name: "Common Workflow Language, Draft 2"
  type: doc
  doc: |
    Authors:

    * Peter Amstutz <peter.amstutz@curoverse.com>, Curoverse
    * Nebojša Tijanić <nebojsa.tijanic@sbgenomics.com>, Seven Bridges Genomics

    Contributers:

    * Luka Stojanovic <luka.stojanovic@sbgenomics.com>, Seven Bridges Genomics
    * John Chilton <jmchilton@gmail.com>, Galaxy Project, Pennsylvania State University
    * Michael R. Crusoe <mcrusoe@msu.edu>, Michigan State University
    * Hervé Ménager <herve.menager@gmail.com>, Institut Pasteur
    * Maxim Mikheev <mikhmv@biodatomics.com>, BioDatomics
    * Stian Soiland-Reyes <soiland-reyes@cs.manchester.ac.uk>, University of Manchester

    # Abstract

    A Workflow is an analysis task represented by a directed graph describing a
    sequence of operations that transform an input data set to output.  This
    specification defines the Common Workflow Language (CWL), a vendor-neutral
    standard for representing workflows and concrete process steps intended to
    be portable across a variety of computing platforms.

    # Status of This Document

    This document is the product of the [Common Workflow Language working
    group](https://groups.google.com/forum/#!forum/common-workflow-language).  The
    latest version of this document is available in the "specification" directory at

    https://github.com/common-workflow-language/common-workflow-language

    The products of the CWL working group (including this document) are made available
    under the terms of the Apache License, version 2.

    # Introduction

    The Common Workflow Language (CWL) working group is an informal, multi-vendor
    working group consisting of various organizations and individuals that have an
    interest in portability of data analysis workflows.  The goal is to create
    specifications like this one that enable data scientists to describe analysis
    tools and workflows that are powerful, easy to use, portable, and support
    reproducibility.

    ## Introduction to draft 2

    This specification represents the second milestone of the CWL group.  Since
    draft-1, this draft introduces a number of major changes and additions:

    * Use of Avro schema (instead of JSON-schema) and JSON-LD for data modeling
    * Significant refactoring of the Command Line Tool description.
    * Data and execution model for Workflows.
    * Extension mechanism though "hints" and "requirements"

    ## Purpose

    CWL is designed to express workflows for data-intensive science, such as
    Bioinformatics, Chemistry, Physics, and Astronomy.  This specification is
    intended to define a data and execution model for Workflows and Command Line
    Tools that can be implemented on top of a variety of computing platforms,
    ranging from individual workstation to cluster, grid, cloud, and high
    performance computing systems.

    ## Dependencies on Other Specifications

    * [JSON](http://json.org)
    * [JSON-LD](http://json-ld.org)
    * [JSON Pointer](https://tools.ietf.org/html/draft-ietf-appsawg-json-pointer-04)
    * [YAML](http://yaml.org)
    * [Avro](https://avro.apache.org)
    * [ECMAScript 5.1 (Javascript)](http://www.ecma-international.org/ecma-262/5.1/)
    * [Uniform Resource Identifier (URI): Generic Syntax](https://tools.ietf.org/html/rfc3986)

    ## Scope

    This document describes the CWL syntax, execution, and object model.  It
    is not intended to document a specific implementation of CWL, however it may
    serve as a reference for the behavior of conforming implementations.

    ## Terminology

    The terminology used to describe CWL documents is defined in the
    Concepts section of the specification. The terms defined in the
    following list are used in building those definitions and in describing the
    actions of an CWL implementation:

    **may**: Conforming CWL documents and CWL implementations are permitted but
    not required to behave as described.

    **must**: Conforming CWL documents and CWL implementations are required to behave
    as described; otherwise they are in error.

    **error**: A violation of the rules of this specification; results are
    undefined. Conforming implementations may detect and report an error and may
    recover from it.

    **fatal error**: A violation of the rules of this specification; results are
    undefined. Conforming implementations must not continue to execute the current
    process and may report an error.

    **at user option**: Conforming software may or must (depending on the modal verb in
    the sentence) behave as described; if it does, it must provide users a means to
    enable or disable the behavior described.

    # Data model

    ## Data concepts

    A **object** is a data structure equivalent to the "object" type in JSON,
    consisting of a unordered set of name/value pairs (referred to here as
    **fields**) and where the name is a string and the value is a string, number,
    boolean, array, or object.

    A **document** is a file containing a serialized object, or an array of objects.

    A **process** is a basic unit of computation which accepts input data,
    performs some computation, and produces output data.

    A **input object** is a object describing the inputs to a invocation of process.

    A **output object** is a object describing the output of an invocation of a process.

    A **input schema** describes the valid format (required fields, data types)
    for an input object.

    A **output schema** describes the valid format for a output object.

    ## Syntax

    Documents containing CWL objects are serialized and loaded using YAML syntax.
    A conforming implementation must accept all valid YAML documents.

    The CWL schema is defined using Avro Linked Data (avro-ld).  Avro-ld is an
    extension of the Apache Avro schema language to support additional
    annotations mapping Avro fields to RDF predicates via JSON-LD.

    A CWL document may be validated by transforming the avro-ld schema to a
    base Apache Avro schema.

    An implementation may interpret a CWL document as
    [JSON-LD](http://json-ld.org) and convert a CWL document to a [Resource
    Definition Framework (RDF)](http://www.w3.org/RDF/) using the JSON-LD
    context extracted from the avro-ld schema.

    The latest draft-2 schema is defined here:
    https://github.com/common-workflow-language/common-workflow-language/blob/master/schemas/draft-2/cwl-avro.yml

    ## Identifiers and references

    If an object contains an `id` field, that is used to uniquely identify the
    object in that document.  The value of the `id` field must be unique over the
    entire document.  The format of the `id` field is that of a [relative fragment
    identifier](https://tools.ietf.org/html/rfc3986#section-3.5), and must start
    with a hash `#` character.

    An implementation may choose to only honor references to object types for
    which the `id` field is explicitly listed in this specification.

    # Execution model

    ## Execution concepts

    A **command line tool** is a process characterized by the
    execution of a standalone, non-interactive program which is invoked on some
    input, produces output, and then terminates.

    A **workflow** is a process characterized by multiple steps, where
    step outputs are connected to the inputs of other downstream
    steps to form a directed graph, and independent steps may run
    concurrently.

    A **runtime environment** is the actual hardware and software environment when
    executing a command line tool.  It includes, but is not limited to, the
    hardware architecture, hardware resources, operating system, software runtime
    (if applicable, such as the Python interpreter or the JVM), libraries, modules,
    packages, utilities, and data files required to run the tool.

    A **workflow platform** is a specific hardware and software and implementation
    capable of interpreting a CWL document and executing the processes specified by
    the document.  The responsibilities of the workflow infrastructure may include
    scheduling process invocation, setting up the necessary runtime environment,
    making input data available, invoking the tool process, and collecting output.

    It is intended that the workflow platform has broad leeway outside of this
    specification to optimize use of computing resources and enforce policies not
    covered by this specifcation.  Some of areas are out of scope for CWL that may
    be handled by a specific workflow platform are:

    * Data security and permissions.
    * Scheduling tool invocations on remote cluster or cloud compute nodes.
    * Using virtual machines or operating system containers to manage the runtime
    (except as described in [Executing tools in
    Docker](#executing-tools-in-docker))
    * Using remote or distributed file systems to manage input and output files.
    * Translating or rewrite file paths.
    * Determining if a process has already been executed and can be skipped and
    re-use previous results.
    * Pause and resume of processes or workflows.

    Conforming CWL documents must not assume anything about the runtime environment
    or workflow platform unless explicitly declared though the use of [process
    requirements](#processrequirement).

    ## Generic execution process

    The generic execution sequence of a CWL process is as follows.

    1. Load and validate CWL document, yielding a process object.
    2. Load input object.
    3. Validate input object against the `inputs` schema for the process.
    4. Validate that process requirements are met.
    5. Perform any further setup required by the specific process type.
    6. Execute the process.
    7. Capture results of process execution into output object.
    8. Validate output object against the `outputs` schema for the process.
    9. Report output object to the process caller.

    ## Requirements and hints

    [Process requirements](#processrequirement) modify the semantics or runtime
    environment of a process.  If an implementation cannot satisfy all
    requirements, or a requirement is listed which is not recognized by the
    implementation, it is a fatal error and the implementation must not attempt
    to run the process, unless overridden at user option.

    A `hint` is similar to a requirement, however it is not an error if an
    implementation cannot satisfy all hints.  The implementation may report a
    warning if a hint cannot be satisfied.

    Requirements are inherited.  A requirement specified in a Workflow applies
    to all workflow steps; a requirement specified on a workflow step will
    apply to the process implementation.  However, when determining how to
    apply a requirement, the closest instance of the requirement is used, that
    is, the requirement on a CommandLineTool will take precendence over the
    workflow step, and a requirement on a workflow step takes precedence over
    the workflow.

    Process requirements are the primary mechanism for specifying extensions to
    the CWL core specification.

    ## Workflow graph

    A workflow describes a set of **steps** and the **dependencies**
    between those processes.  When a process produces output that will be
    consumed by a second process, the second process is a dependency of the
    first process.  When there is a dependency, the workflow engine must must
    execute the producer process first and wait for it to successfully complete
    and produce output before the executing the dependent second process that
    will consume the output.  If two processes are defined in the workflow
    graph that are not directly or indirectly dependent, these processes are
    **independent**, and may execute in any order or execute concurrently.  A
    workflow is complete when all steps have been executed.

    ## Success and failure

    A completed process must result in one of `success`, `temporaryFailure` or
    `permanentFailure` states.  An implementation may choose to retry a process
    execution which resulted in `temporaryFailure`.  An implementation may
    choose to either continue running other steps of a workflow, or terminate
    immediately upon permanentFailure.

    * If any step of a workflow execution results in `permanentFailure`, the
    workflow status is `permanentFailure`.

    * If one or more steps result in `temporaryFailure` and all other steps
    complete `success` or are not executed, then the workflow status is
    `temporaryFailure`.

    * If all workflow steps are executed and complete with `success` then the workflow
    status is `success`.

    ## Expressions

    An expression is a fragment of executable code which is evaluated by
    workflow platform to affect the inputs, outputs, or behavior of a process.
    In the generic execution sequence, expressions may be evaluated during step
    5 (process setup), step 6 (execute process), and/or step 7 (build output).
    Expressions are distinct from regular processes in that they are intended
    to modify the workflow

    An implementation must provide the predefined `cwl:JsonPointer` expression
    engine.  This expression engine specifies a [JSON
    Pointer](https://tools.ietf.org/html/draft-ietf-appsawg-json-pointer-04)
    into an expression input object consisting of the **job** and **context**
    fields described below.

    An expression engine defined with
    [ExpressionEngineRequirement](#expressionenginerequirement) is a command
    line program following the following protocol:

      * On standard input, receive a JSON object with the following fields:

        - **expressionDefs**: A list of strings from the `expressionDefs` field, or
          `null` if `expressionDefs` is not specified.

        - **job**: The input object of the current Process (context dependent).

        - **context**: The specific value being transformed (context dependent).  May
          be null.

        - **script**: The code fragment to evaluate.

      * On standard output, print a single JSON value (string, number, array, object,
        boolean, or null) for the return value.

    Expressions must be evaluated in an isolated context (a "sandbox") which
    permits no side effects to leak outside the context, and permit no outside
    data to leak into the context.

    Implementations may apply limits, such as process isolation, timeouts, and
    operating system containers/jails to minimize the security risks associated
    with running untrusted code.

    The order in which expressions are evaluated within a process or workflow
    is undefined.


    # Sample CWL workflow

    revtool.cwl:
    ```
    #
    # Simplest example command line program wrapper for the Unix tool "rev".
    #
    class: CommandLineTool
    description: "Reverse each line using the `rev` command"

    # The "inputs" array defines the structure of the input object that describes
    # the inputs to the underlying program.  Here, there is one input field
    # defined that will be called "input" and will contain a "File" object.
    #
    # The input binding indicates that the input value should be turned into a
    # command line argument.  In this example inputBinding is an empty object,
    # which indicates that the file name should be added to the command line at
    # a default location.
    inputs:
      - id: "#input"
        type: File
        inputBinding: {}

    # The "outputs" array defines the structure of the output object that
    # describes the outputs of the underlying program.  Here, there is one
    # output field defined that will be called "output", must be a "File" type,
    # and after the program executes, the output value will be the file
    # output.txt in the designated output directory.
    outputs:
      - id: "#output"
        type: File
        outputBinding:
          glob: output.txt

    # The actual program to execute.
    baseCommand: rev

    # Specify that the standard output stream must be redirected to a file called
    # output.txt in the designated output directory.
    stdout: output.txt
    ```

    sorttool.cwl:
    ```
    # Example command line program wrapper for the Unix tool "sort"
    # demonstrating command line flags.
    class: CommandLineTool
    description: "Sort lines using the `sort` command"

    # This example is similar to the previous one, with an additional input
    # parameter called "reverse".  It is a boolean parameter, which is
    # intepreted as a command line flag.  The value of "prefix" is used for
    # flag to put on the command line if "reverse" is true, if "reverse" is
    # false, no flag is added.
    #
    # This example also introduced the "position" field.  This indicates the
    # sorting order of items on the command line.  Lower numbers are placed
    # before higher numbers.  Here, the "--reverse" flag (if present) will be
    # added to the command line before the input file path.
    inputs:
      - id: "#reverse"
        type: boolean
        inputBinding:
          position: 1
          prefix: "--reverse"
      - id: "#input"
        type: File
        inputBinding:
          position: 2

    outputs:
      - id: "#output"
        type: File
        outputBinding:
          glob: output.txt

    baseCommand: sort
    stdout: output.txt
    ```

    revsort.cwl:
    ```
    #
    # This is a two-step workflow which uses "revtool" and "sorttool" defined above.
    #
    class: Workflow
    description: "Reverse the lines in a document, then sort lines."

    # Requirements specify prerequisites and extensions to the workflow.
    # In this example, DockerRequirement specifies a default Docker container
    # in which the command line tools will execute.
    requirements:
      - class: DockerRequirement
        dockerPull: debian:8

    # The inputs array defines the structure of the input object that describes
    # the inputs to the workflow.
    #
    # The "reverse_sort" input parameter demonstrates the "default" field.  If the
    # field "reverse_sort" is not provided in the input object, the default value will
    # be used.
    inputs:
      - id: "#input"
        type: File
        description: "The input file to be processed."
      - id: "#reverse_sort"
        type: boolean
        default: true
        description: "If true, reverse (decending) sort"

    # The "outputs" array defines the structure of the output object that describes
    # the outputs of the workflow.
    #
    # Each output field must be connected to the output of one of the workflow
    # steps using the "connect" field.  Here, the parameter "#output" of the
    # workflow comes from the "#sorted" output of the "sort" step.
    outputs:
      - id: "#output"
        type: File
        connect: { source: "#sorted" }
        description: "The output with the lines reversed and sorted."

    # The "steps" array lists the executable steps that make up the workflow.
    # The tool to execute each step is listed in the "run" field.
    #
    # In the first step, the "inputs" field of the step connects the upstream
    # parameter "#input" of the workflow to the input parameter of the tool
    # "revtool.cwl#input"
    #
    # In the second step, the "inputs" field of the step connects the output
    # parameter "#reversed" from the first step to the input parameter of the
    # tool "sorttool.cwl#input".
    steps:
      - inputs:
          - { param: "revtool.cwl#input", connect: { source: "#input" } }
        outputs:
          - { id: "#reversed", param: "revtool.cwl#output" }
        run: { import: revtool.cwl }

      - inputs:
          - { param: "sorttool.cwl#input", connect: { source: "#reversed" } }
          - { param: "sorttool.cwl#reverse", connect: { source: "#reverse_sort" } }
        outputs:
          - { id: "#sorted", param: "sorttool.cwl#output" }
        run: { import: sorttool.cwl }
    ```

    Sample input object:
    ```
    {
      "input": {
        "class": "File",
        "path": "whale.txt"
      }
    }
    ```

    Sample output object:
    ```
    {
        "output": {
            "path": "/tmp/tmpdeI_p_/output.txt",
            "size": 1111,
            "class": "File",
            "checksum": "sha1$b9214658cc453331b62c2282b772a5c063dbd284"
        }
    }
    ```

  jsonldPrefixes: {
    "cwl": "http://github.com/common-workflow-language#",
    "avro": "http://github.com/common-workflow-language/avro#",
    "wfdesc": "http://purl.org/wf4ever/wfdesc#",
    "dct": "http://purl.org/dc/terms/",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#"
  }
  jsonldVocab: cwl


- name: Reference
  type: doc
  doc: This section specifies the core object types that make up a CWL document.


- name: Datatype
  type: enum
  docParent: Schema
  symbols:
    - "null"
    - boolean
    - int
    - long
    - float
    - double
    - bytes
    - string
    - record
    - enum
    - array
    - map
    - File
    - Any
  jsonldPrefix: avro
  jsonldPredicate:
    - symbol: File
      predicate: "cwl:File"
  doc: |
   CWL data types are based on Avro schema declarations.  Refer to the [Avro
   schema declaration
   documentation](https://avro.apache.org/docs/current/spec.html#schemas) for
   detailed information.  In addition, CWL defines [`File`](#file)
   as a special record type.

   ## Primitive types

   * **null**: no value
   * **boolean**: a binary value
   * **int**: 32-bit signed integer
   * **long**: 64-bit signed integer
   * **float**: single precision (32-bit) IEEE 754 floating-point number
   * **double**: double precision (64-bit) IEEE 754 floating-point number
   * **bytes**: sequence of 8-bit unsigned bytes
   * **string**: unicode character sequence

   ## Complex types

   * **record**: An object with one or more fields defined by name and type
   * **enum**: A value from a finite set of symbolic values
   * **array**: An ordered sequence of values
   * **map**: An unordered collection of key/value pairs

   ## Any type

   The `Any` type validates for any non-null value.

- name: File
  type: record
  docParent: Datatype
  fields:
    - name: "class"
      type:
        type: enum
        name: "File_class"
        symbols:
          - File
      jsonldPredicate:
        "@id": "@type"
        "@type": "@vocab"
    - name: "path"
      type: "string"
    - name: "checksum"
      type: ["null", "string"]
    - name: "size"
      type: ["null", "long"]
    - name: "secondaryFiles"
      type:
        - "null"
        - type: array
          items: File


# - name: Import
#   type: record
#   fields:
#     - name: "import"
#       type: "string"
#       jsonldPredicate: "@id"
#   doc: |
#     A URI reference to external document or document fragment.  This may refer
#     to an object within the current document, to an external file, or to a
#     labeled object within an external file.  To reference an object within the
#     current document or an external document, the object must be labeled with
#     an `id` field.


- name: DataLink
  type: record
  docAfter: WorkflowStep
  doc: |
    A data link connects the value of one parameter to another such that when a
    value becomes available for the parameter specified by `source`, that value
    of the parameter is propagated to the destination parameter.
  fields:
    - name: source
      type: string
      jsonldPredicate:
        "@id": "wfdesc:hasSource"
        "@type": "@id"
      doc: |
        A workflow parameter, either an input parameter of the workflow or an
        output parameter of another workflow step.


- name: Schema
  type: record
  doc: "A schema defines a parameter type."
  docParent: Parameter
  fields:
    - name: type
      doc: "The data type of this parameter."
      type:
        - "Datatype"
        - "Schema"
        - "string"
        - type: "array"
          items: [ "Datatype", "Schema", "string" ]
      jsonldPredicate:
        "@id": "avro:type"
        "@type": "@vocab"
    - name: fields
      type:
        - "null"
        - type: "array"
          items: "Schema"
      jsonldPredicate:
        "@id": "avro:fields"
        "@container": "@list"
      doc: "When `type` is `record`, defines the fields of the record."
    - name: "symbols"
      type:
        - "null"
        - type: "array"
          items: "string"
      jsonldPredicate:
        "@id": "avro:symbols"
        "@container": "@list"
      doc: "When `type` is `enum`, defines the set of valid symbols."
    - name: items
      type:
        - "null"
        - "Datatype"
        - "Schema"
        - "string"
        - type: "array"
          items: [ "Datatype", "Schema", "string" ]
      jsonldPredicate:
        "@id": "avro:items"
        "@container": "@list"
      doc: "When `type` is `array`, defines the type of the array elements."
    - name: "values"
      type:
        - "null"
        - "Datatype"
        - "Schema"
        - "string"
        - type: "array"
          items: [ "Datatype", "Schema", "string" ]
      jsonldPredicate:
        "@id": "avro:values"
        "@container": "@list"
      doc: "When `type` is `map`, defines the value type for the key/value pairs."


- name: Parameter
  type: record
  docParent: Process
  abstract: true
  doc: |
    Define an input or output parameter to a process.

  fields:
    - name: type
      type:
        - "null"
        - Datatype
        - Schema
        - type: array
          items:
            - Datatype
            - Schema
      jsonldPredicate:
        "@id": "avro:type"
        "@type": "@vocab"
      doc: |
        Specify valid types of data that may be assigned to this parameter.
    - name: label
      type:
        - "null"
        - string
      jsondldPredicate: "rdfs:label"
      doc: "A short, human-readable label of this parameter object."
    - name: description
      type:
        - "null"
        - string
      jsondldPredicate: "rdfs:comment"
      doc: "A long, human-readable description of this parameter object."

    # - name: "default"
    #   type:
    #     - "null"
    #   doc: |
    #     The default value for this parameter if there is no `connect`
    #     field.

- name: JsonPointer
  type: enum
  docParent: Expression
  symbols:
    - "JsonPointer"
  jsonldPrefix: "cwl"

- type: record
  name: Expression
  docAfter: ExpressionTool
  doc: |
    See [ExpressionEngineRequirement](#expressionenginerequirement)
    for information on how to define a expression engine.
  fields:
    - name: engine
      type:
        - JsonPointer
        - string
      doc: |
        Either `cwl:JsonPointer` or a reference to an
        ExpressionEngineRequirement defining which engine to use.
      jsonldPredicate:
        "@id": "cwl:engine"
        "@type": "@id"
    - name: script
      type: string
      doc: "The code to be executed by the expression engine."


- name: Binding
  type: record
  docParent: Parameter
  fields:
    - name: loadContents
      type:
        - "null"
        - boolean
      doc: |
        Read up to the first 64 KiB of text from the file and place it in the
        "contents" field of the file object for manipulation by expressions.
    - name: streamable
      type: ["null", "boolean"]
      doc: |
        Only applies when `type` is `File`.  A value of `true` indicates that the file will
        be read or written sequentially without seeking.  Default: `false`.
    - name: secondaryFiles
      type:
        - "null"
        - "string"
        - Expression
        - type: "array"
          items: ["string", "Expression"]
      doc: |
        Applies when `type` is `File`.  Describes files that must be
        included alongside the primary file.


- name: InputSchema
  type: record
  extends: Schema
  docParent: InputParameter
  specialize: {Schema: InputSchema}
  fields:
    - name: inputBinding
      type: [ "null", "Binding" ]
      doc: |
        Describes how to handle a value in the input object convert it
        into a concrete form for execution, such as command line parameters.


- name: OutputSchema
  type: record
  extends: Schema
  docParent: OutputParameter
  specialize: {Schema: OutputSchema}
  fields:
    - name: "outputBinding"
      type: [ "null", "Binding" ]
      doc: |
        Describes how to handle the concrete outputs of a process (such as
        files created by a programs) and describe them in the process output
        parameter.


- name: InputParameter
  type: record
  extends: Parameter
  docAfter: Parameter
  specialize: {Schema: InputSchema}
  fields:
    - name: id
      type: string
      jsonldPredicate: "@id"
      doc: "The unique identifier for this parameter object."
    - name: "inputBinding"
      type: [ "null", "Binding" ]
      doc: |
        Describes how to handle the inputs of a process and convert them
        into a concrete form for execution, such as command line parameters.

- name: OutputParameter
  type: record
  extends: Parameter
  docAfter: Parameter
  specialize: {Schema: OutputSchema}
  fields:
    - name: id
      type: string
      jsonldPredicate: "@id"
      doc: "The unique identifier for this parameter object."
    - name: "outputBinding"
      type: [ "null", "Binding" ]
      doc: |
        Describes how to handle the concrete outputs of a process step (such as
        files created by a program) and describe them in the process output
        parameter.

- type: record
  name: ProcessRequirement
  docAfter: ExpressionTool
  doc: |
    A process requirement declares a prerequisite that may or must be fufilled
    before executing a process.  See [`Process.hints`](#process) and
    [`Process.requirements`](#process).

    Process requirements are the primary mechanism for specifying extensions to
    the CWL core specification.

  fields:
    - name: "class"
      type: "string"
      doc: "The specific requirement type."
      jsonldPredicate:
        "@id": "@type"
        "@type": "@vocab"


- type: record
  name: Process
  abstract: true
  docAfter: ProcessRequirement
  doc: |

    The base executable type in CWL is the `Process` object defined by the
    document.  Note that the `Process` object is abstract and cannot be
    directly executed.

  fields:
    - name: id
      type: ["null", string]
      jsonldPredicate: "@id"
      doc: "The unique identifier for this process object."
    - name: inputs
      type:
        type: array
        items: InputParameter
      jsonldPredicate: "wfdesc:hasInput"
      doc: |
        Defines the input parameters of the process.  The process is ready to
        run when all required input parameters are associated with concrete
        values.  Input parameters include a schema for each parameter and is
        used to validate the input object, it may also be used build a user
        interface for constructing the input object.
    - name: outputs
      type:
        type: array
        items: OutputParameter
      jsonldPredicate: "wfdesc:hasOutput"
      doc: |
        Defines the parameters representing the output of the process.  May be
        used to generate and/or validate the output object.
    - name: requirements
      type:
        - "null"
        - type: array
          items: ProcessRequirement
      doc: >
        Declares requirements that apply to either the runtime environment or the
        workflow engine that must be met in order to execute this process.  If
        an implementation cannot satisfy all requirements, or a requirement is
        listed which is not recognized by the implementation, it is a fatal
        error and the implementation must not attempt to run the process,
        unless overridden at user option.
    - name: hints
      type:
        - "null"
        - type: array
          items: ProcessRequirement
      doc: >
        Declares hints applying to either the runtime environment or the
        workflow engine that may be helpful in executing this process.  It is
        not an error if an implementation cannot satisfy all hints, however
        the implementation may report a warning.
    - name: label
      type:
        - "null"
        - string
      jsondldPredicate: "rdfs:label"
      doc: "A short, human-readable label of this process object."
    - name: description
      type:
        - "null"
        - string
      jsondldPredicate: "rdfs:comment"
      doc: "A long, human-readable description of this process object."

- type: record
  name: CommandLineBinding
  extends: Binding
  docParent: CommandInputParameter
  doc: |

    When listed under `inputBinding` in the input schema, the term
    "value" refers to the the corresponding value in the input object.  For
    binding objects listed in `CommandLineTool.arguments`, the term "value"
    refers to the effective value after evaluating `valueFrom`.

    The binding behavior when building the command line depends on the data
    type of the value.  If there is a mismatch between the type described by
    the input schema and the effective value, such as resulting from an
    expression evaluation, an implementation must use the data type of the
    effective value.

      - **string**: Add `prefix` and the string to the command line.

      - **number**: Add `prefix` and decimal representation to command line.

      - **boolean**: If true, add `prefix` to the command line.  If false, add
          nothing.

      - **File**: Add `prefix` and the value of
        [`File.path`](#file) to the command line.

      - **array**: If `itemSeparator` is specified, add `prefix` and the join
          the array into a single string with `itemSeparator` separating the
          items.  Otherwise add `prefix` and recursively add individual
          elements.

      - **object**: Add `prefix` only, and recursively add object fields for
          which `inputBinding` is specified.

      - **null**: Add nothing.

  fields:
    - name: "position"
      type: ["null", "int"]
      doc: "The sorting key.  Default position is 0."
    - name: "prefix"
      type: [ "null", "string"]
      doc: "Command line prefix to add before the value."
    - name: "separate"
      type: ["null", boolean]
      doc: |
        If true (default) then the prefix and value must be added as separate
        command line arguments; if false, prefix and value must be concatenated
        into a single command line argument.
    - name: "itemSeparator"
      type: ["null", "string"]
      doc: |
        Join the array elements into a single string with the elements
        separated by by `itemSeparator`.
    - name: "valueFrom"
      type:
        - "null"
        - "string"
        - "Expression"
      doc: |
        If `valueFrom` is a constant string value, use this as the value and
        apply the binding rules above.

        If `valueFrom` is an expression, evaluate the expression to yield the
        actual value to use to build the command line and apply the binding
        rules above.  If the inputBinding is associated with an input
        parameter, the "context" of the expression will be the value of the
        input parameter.

        When a binding is part of the `CommandLineTool.arguments` field,
        the `valueFrom` field is required.


- type: record
  name: CommandOutputBinding
  extends: Binding
  docParent: CommandOutputParameter
  doc: |
    Describes how to generate an output parameter based on the files produced
    by a CommandLineTool.

    The output parameter is generated by applying these operations in
    the following order:

      - glob
      - loadContents
      - outputEval
  fields:
    - name: glob
      type:
        - "null"
        - string
        - Expression
        - type: array
          items: string
      doc: |
        Find files relative to the output directory, using POSIX glob(3)
        pathname matching.  If provided an array, match all patterns in the
        array.  If provided an expression, the expression must return a string
        or an array of strings, which will then be evaluated as a glob pattern.
        Only files which actually exist will be matched and returned.

    - name: outputEval
      type:
        - "null"
        - Expression
      doc: |
        Evaluate an expression to generate the output value.  If `glob` was
        specified, the script `context` will be an array containing any files that were
        matched.  Additionally, if `loadContents` is `true`, the file objects
        will include up to the first 64 KiB of file contents in the `contents` field.


- type: record
  name: CommandInputSchema
  extends: InputSchema
  docParent: CommandInputParameter
  specialize:
    InputSchema: CommandInputSchema
    Binding: CommandLineBinding


- type: record
  name: CommandOutputSchema
  extends: OutputSchema
  docParent: CommandOutputParameter
  specialize:
    OutputSchema: CommandOutputSchema
    Binding: CommandOutputBinding


- type: record
  name: CommandInputParameter
  extends: InputParameter
  docParent: CommandLineTool
  doc: An input parameter for a CommandLineTool.
  specialize:
    InputSchema: CommandInputSchema
    Binding: CommandLineBinding


- type: record
  name: CommandOutputParameter
  extends: OutputParameter
  docParent: CommandLineTool
  doc: An output parameter for a CommandLineTool.
  specialize:
    OutputSchema: CommandOutputSchema
    Binding: CommandOutputBinding

- type: record
  name: CommandLineTool
  extends: Process
  docAfter: Workflow
  specialize:
    InputParameter: CommandInputParameter
    OutputParameter: CommandOutputParameter
  doc: |

    A CommandLineTool process is a process implementation for executing a
    standalone, non-interactive command line applications.  To help accomodate
    of the enormous variety in syntax and semantics for input, runtime
    environment, invocation, and output of arbitrary programs, CommandLineTool
    provides the concept of "input binding" to describe how to translate input
    parameters to an actual program invocation, and "output binding" to
    describe how generate output parameters from program output.

    # Input binding

    The tool command line is built by applying command line bindings to the
    input object.  Bindings are listed either as part of an [input
    parameter](#commandlineinputparameter) using the `inputBinding` field, or
    separately using the `arguments` field of the CommandLineTool.

    The algorithm to build the command line is as follows.  In this algorithm,
    the sort key is a list consisting of one or more numeric and string elements.

      1. Collect `CommandLineBinding` objects from `arguments`.  Assign a sorting
      key `[position, i]` where `position` is
      [`CommandLineBinding.position`](#commandlinebinding) and the `i`
      is the index in the `arguments` list.

      2. Collect `CommandLineBinding` objects from the `inputs` schema and
      associate them with values from the input object.  Where the input type
      is a record, array, or map, recursively walk the schema and input object,
      collecting nested `CommandLineBinding` objects and associating them with
      values from the input object.

      3. Assign a sorting key for each leaf binding object by appending nested
      `position` fields together with the array index, or map key of the data
      at each nesting level.  If two bindings have the same position, the tie
      must be broken using the lexographic ordering of the field or parameter
      name immediately containing the binding.

      4. Sort elements lexicographically using the assigned sorting keys.
      Numeric entries sort before strings.  Strings are sorted based on UTF-8
      encoding.

      5. In the sorted order, apply the rules defined in
      [`CommandLineBinding`](#commandlinebinding) to convert bindings to actual
      command line elements.

      6. Insert elements from `baseCommand` at the beginning of the command
      line.

    # Runtime environment

    All files listed in the input object must be made available in the runtime
    environment.  The implementation may use a shared or distributed file
    system or transfer files via explicit download.  Implementations may choose
    not to provide access to files not explicitly specified by the input object
    or process requirements.

    Output files produced by tool execution must be written to the **designated
    output directory**.

    The initial current working directory when executing the tool must be the
    designated output directory.

    The `TMPDIR` environment variable must be set in the runtime environment to
    the **designated temporary directory**.  Any files written to the
    designated temporary directory may be deleted by the workflow
    infrastructure when the tool invocation is complete.

    An implementation may forbid the tool from writing to any location in the
    runtime environment file system other than the designated temporary
    directory and designated output directory.  An implementation may provide
    read-only input files, and disallow in-place update of input files.

    The standard input stream and standard output stream may be redirected as
    described in the `stdin` and `stdout` fields.

    ## Extensions

    [DockerRequirement](#dockerrequirement),
    [CreateFileRequirement](#createfilerequirement), and
    [EnvVarRequirement](#envvarrequirement), are available as standard
    extensions to core command line tool semantics for defining the runtime
    environment.

    # Execution

    Once the command line is built and the runtime environment is created, the
    actual tool is executed.

    The standard error stream and standard output stream (unless redirected by
    setting `stdout`) may be captured by platform logging facilities for
    storage and reporting.

    Tools may be multithreaded or spawn child processes; however, when the
    parent process exits, the tool is considered finished regardless of whether
    any detached child processes are still running.  Tools must not require any
    kind of console, GUI, or web based user interaction in order to start and
    run to completion.

    The exit code of the process indicates if the process completed
    successfully.  By convention, an exit code of zero is treated as success
    and non-zero exit codes are treated as failure.  This may be customized by
    providing the fields `successCodes`, `temporaryFailCodes`, and
    `permanentFailCodes`.  An implementation may choose to default unspecified
    non-zero exit codes to either `temporaryFailure` or `permanentFailure`.

    # Output binding

    If the output directory contains a file called "cwl.output.json", that file
    must be loaded and used as the output object.  Otherwise, the output object
    must be generated by walking the parameters listed in `outputs` and
    applying output bindings to the tool output.  Output bindings are
    associated with output parameters using the `outputBinding` field.  See
    [`CommandOutputBinding`](#commandoutputbinding) for details.

  fields:
    - name: "class"
      jsonldPredicate:
        "@id": "@type"
        "@type": "@vocab"
      type: string
    - name: baseCommand
      doc: |
        Specifies the program to execute.  If the value is an array, the first
        element is the program to execute, and subsequent elements are placed
        at the beginning of the command line in prior to any command line
        bindings.  If the program includes a path separator character it must
        be an absolute path, otherwise it is an error.  If the program does not
        include a path separator, search the `$PATH` variable in the runtime
        environment find the absolute path of the executable.
      type:
        - string
        - type: array
          items: string
      jsonldPredicate:
        "@id": "cwl:baseCommand"
        "@container": "@list"
    - name: arguments
      doc: |
        Command line bindings which are not directly associated with input parameters.
      type:
        - "null"
        - type: array
          items: [string, CommandLineBinding]
      jsonldPredicate:
        "@id": "cwl:arguments"
        "@container": "@list"
    - name: stdin
      type: ["null", string, Expression]
      doc: |
        A path to a file whose contents must be piped into the command's
        standard input stream.
    - name: stdout
      type: ["null", string, Expression]
      doc: |
        Capture the command's standard output stream to a file written to
        the designated output directory.

        If `stdout` is a string, it specifies the file name to use.

        If `stdout` is an expression, the expression is evaluated and must
        return a string with the file name to use to capture stdout.  If the
        return value is not a string, or the resulting path contains illegal
        characters (such as the path separator `/`) it is an error.
    - name: successCodes
      type:
        - "null"
        - type: array
          items: int
      doc: |
        Exit codes that indicate the process completed successfully.

    - name: temporaryFailCodes
      type:
        - "null"
        - type: array
          items: int
      doc: |
        Exit codes that indicate the process failed due to a possibly
        temporary condition, where excuting the process with the same
        runtime environment and inputs may produce different results.

    - name: permanentFailCodes
      type:
        - "null"
        - type: array
          items: int
      doc:
        Exit codes that indicate the process failed due to a permanent logic
        error, where excuting the process with the same runtime environment and
        same inputs is expected to always fail.

- type: record
  name: ExpressionTool
  extends: Process
  docAfter: CommandLineTool
  doc: |
    Execute an expression as a process step.
  fields:
    - name: "class"
      jsonldPredicate:
        "@id": "@type"
        "@type": "@vocab"
      type: string
    - name: expression
      type: Expression
      doc: |
        The expression to execute.  The expression must return a JSON object which
        matches the output parameters of the ExpressionTool.


- name: WorkflowOutputParameter
  type: record
  extends: OutputParameter
  docParent: Workflow
  doc: |
    Describe an output parameter of a workflow.  The parameter must be
    connected to one or more parameters defined in the workflow that will
    provide the value of the output parameter.
  fields:
    - name: connect
      doc: |
        Connect this parameter to one or parameters in the workflow
        which will provide the workflow output.
      jsonldPredicate:
        "@reverse": "wfdesc:hasSink"
        "@type": "@id"
      type:
        - "null"
        - DataLink
        - type: array
          items: DataLink


- type: record
  name: WorkflowStepInput
  extends: Parameter
  docParent: WorkflowStep
  doc: |
    The input of a workflow step connects an upstream parameter (from the
    workflow inputs, or the outputs of other workflows steps) with the input
    parameters of the underlying process.
  fields:
    - name: id
      type: ["null", string]
      jsonldPredicate: "@id"
      doc: "A unique identifier for this workflow input parameter."
    - name: param
      type: string
      jsonldPredicate:
        "@id": "wfdesc:hasArtifact"
        "@type": "@id"
      doc: |
        The input parameter of the underlying process (specified in the `run` field
        of `WorkflowStep`) that will get the value from `connect`.
    - name: connect
      doc: |
        Specifies one or more workflow parameters that will provide input to the underlying process parameter specified in `param`.
      jsonldPredicate:
        "@reverse": "wfdesc:hasSink"
        "@type": "@id"
      type:
        - "null"
        - DataLink
        - type: array
          items: DataLink


- type: record
  name: WorkflowStepOutput
  docParent: WorkflowStep
  extends: Parameter
  doc: |
    Associate an output parameter of the underlying process with a workflow
    parameter.  The workflow parameter (given in the `id` field) be may be used
    as a `source` to connect with input parameters of other workflow steps, or
    with an output parameter of the process.
  fields:
    - name: id
      type: string
      jsonldPredicate: "@id"
      doc: |
        A unique identifier for this workflow output parameter.  This is the
        identifier to use in the `source` field of `DataLink` to connect the
        output value to downstream parameters.
    - name: param
      type: string
      jsonldPredicate:
        "@id": "wfdesc:hasArtifact"
        "@type": "@id"
      doc: |
        The output parameter of the underlying process (specified in the `run` field
        of `WorkflowStep`) that will provide the value for this this parameter.

- name: WorkflowStep
  type: record
  extends: Process
  docParent: Workflow
  specialize:
    InputParameter: WorkflowStepInput
    OutputParameter: WorkflowStepOutput
  doc: |
    A workflow step is the basic executable element of a workflow.  It specifies the
    underlying process implementation (such as `CommandLineTool`) in the `run`
    field and connects the input and output parameters of the underlying
    process to workflow parameters.
  fields:
    - name: run
      type: [CommandLineTool, ExpressionTool]
      doc: "Specifies the process to run."


- name: Workflow
  type: record
  docParent: Reference
  extends: Process
  specialize:
    OutputParameter: WorkflowOutputParameter
  doc: |
    A workflow is a process consisting of one or more `steps`.  Each
    step has input and output parameters defined by the `inputs` and `outputs`
    fields.  A workflow executes as described in [execution model](#workflow_graph).

    # Dependencies

    Dependencies between parameters are expressed by [data links](#datalink)
    on [workflow step input parameters](#workflowstepinput)
    and [workflow output parameters](#workflowoutputparameter).

    A data link expresses the dependency of one parameter on another such that
    when a value is associated with the parameter specified by
    [`DataLink.source`](#datalink), that value is propagated to the destination
    parameter.  When all data links inbound to a given step are fufilled, the
    step is ready to execute.

    ## Multiple inbound data links

    There may be multiple inbound data links sending data to a parameter (the
    "sink").  If so, the following rules apply to combine the inputs:

      1. The sink parameter type must be an array, or named in a [workflow scatter](#scatter)
         operation.
      2. The source and sink parameters must be compatible types, or the source
         type must be compatible with single element type "items" type of the
         "sink" array type.
      3. Source parameters which are array of the same type are concatenated;
         source parameters which are single element types are appended added as
         single elements.

    # Extensions

    [Scatter](#scatter) and [Subworkflows](#Subworkflows) are available
    as standard extensions to core workflow semantics.

  fields:
    - name: "class"
      jsonldPredicate:
        "@id": "@type"
        "@type": "@vocab"
      type: string
    - name: steps
      jsonldPredicate: "wfdesc:hasSubProcess"
      doc: |
        The individual steps that make up the workflow.  Steps are executed when all
        input data links are fufilled.  An implementation may choose to execute
        the steps in a different order than listed and/or execute steps
        concurrently, provided that dependencies between steps are met.
      type:
        - type: array
          items: WorkflowStep


- type: record
  name: DockerRequirement
  extends: ProcessRequirement
  doc: |
    Indicates that a workflow component should be run in a
    [Docker](http://docker.com) container, and specifies how to fetch or build
    the image.

    If a CommandLineTool lists [`DockerRequirement`](#dockerrequirement) under
    `hints` or `requirements`, it may (or must) be run in the specified Docker
    container.

    The platform must first acquire or install the correct Docker image, as
    described by [`DockerRequirement`](#dockerrequirement).

    The platform must execute the tool in the container using `docker run` with
    the appropriate Docker image and the tool command line.

    The workflow platform may provide input files and the designated output
    directory through the use of volume bind mounts.  The platform may rewrite
    file paths in the input object to correspond to the Docker bind mounted
    locations.

    When running a tool contained in Docker, the workflow platform must not
    assume anything about the contents of the Docker container, such as the
    presence or absence of specific software, except to assume that the
    generated command line represents a valid command within the runtime
    environment of the container.

  fields:
    - name: dockerPull
      type: ["null", "string"]
      doc: "Get a Docker image using `docker pull`."
    - name: "dockerLoad"
      type: ["null", "string"]
      doc: "Specify a HTTP URL from which to download a Docker image using `docker load`."
    - name: dockerFile
      type: ["null", "string"]
      doc: "Supply the contents of a Dockerfile which will be build using `docker build`."
    - name: dockerImageId
      type: ["null", "string"]
      doc: |
        The image id that will be used for `docker run`.  May be a
        human-readable image name or the image identifier hash.  May be skipped
        if `dockerPull` is specified, in which case the `dockerPull` image id
        will be used.
    - name: dockerOutputDirectory
      type: ["null", "string"]
      doc: |
        Set the designated output directory to a specific location inside the
        Docker container.


- name: Subworkflow
  type: record
  extends: Process
  docParent: Subworkflows
  specialize:
    InputParameter: WorkflowStepInput
    OutputParameter: WorkflowStepOutput
  doc: |
    A nested workflow to run as part of a larger workflow.  The workflow is
    specified in the `run` field and connects the input and output parameters
    of the nested workflow to workflow parameters.
  fields:
    - name: run
      type: Workflow
      doc: "Specifies the nested workflow to run."


- type: record
  name: Subworkflows
  extends: ProcessRequirement
  doc: |
    Specify one or more subworkflows to execute alongside the process steps.
    Only valid when in the requirements list of a [Workflow](#workflow).
  fields:
    - name: subworkflows
      type:
        type: array
        items: Subworkflow
      doc: "A list of subworkflows to execute as part of the overall parent workflow."


- type: record
  name: "FileDef"
  docParent: CreateFileRequirement
  doc: |
    Define a file that must be placed by in the designated output directory
    prior to executing the command line tool.  May be the result of executing
    an expression, such as building a configuration file from a template.
  fields:
    - name: "filename"
      type: ["string", "Expression"]
      doc: "The name of the file to create in the output directory."
    - name: "fileContent"
      type: ["string", "Expression"]
      doc: |
        If the value is a string literal or an expression which evalutes to a
        string, a new file must be created with the string as the file contents.

        If the value is an expression that evaluates to a File object, this
        indicates the referenced file should be added to the designated output
        directory prior to executing the tool.

        Files added in this way may be read-only, and may be implemented
        through bind mounts or file system links in such a way as to avoid
        unecessary copying of the input file.


- name: CreateFileRequirement
  type: record
  extends: ProcessRequirement
  doc: |
    Define a list of files that must be created and placed by the workflow
    platform in the designated output directory prior to executing the command
    line tool.  See `FileDef` for details.
  fields:
    - name: fileDef
      type:
        type: "array"
        items: "FileDef"
      doc: The list of files


- type: record
  name: EnvironmentDef
  docParent: EnvVarRequirement
  doc: |
    Define an environment variable that will be set in the runtime environment
    by the workflow platform when executing the command line tool.  May be the
    result of executing an expression, such as getting a parameter from input.
  fields:
    - name: "envName"
      type: "string"
      doc: The environment variable name
    - name: "envValue"
      type: ["string", "Expression"]
      doc: The environment variable value


- name: EnvVarRequirement
  type: record
  extends: ProcessRequirement
  doc: |
    Define a list of environment variables which will be set in the
    execution environment of the tool.  See `EnvironmentDef` for details.
  fields:
    - name: envDef
      type:
        type: "array"
        items: "EnvironmentDef"
      doc: The list of environment variables.


- name: ScatterMethod
  type: enum
  docParent: Scatter
  doc: The scatter method, as defined in [Scatter](#scatter).
  symbols:
    - dotproduct
    - nested_crossproduct
    - flat_crossproduct

- name: Scatter
  type: record
  extends: ProcessRequirement
  doc: |
    Only valid when declared on a [WorkflowStep](#workflowstep) or
    [Subworkflow](#subworkflow).

    A "scatter" operation specifies that the associated workflow step or
    subworkflow should execute separately over a list of input elements.  Each
    job making up a scatter operaution is independent and may be executed
    concurrently.

    The `scatter` field specifies one or more input parameters which will be
    scattered.  An input parameter may be listed more than once.  The declared
    type of each input parameter is implicitly wrapped in an array for each
    time it appears in the `scatter` field.  As a result, upstream parameters
    which are connected to scattered parameters may be arrays.

    All output parameters types are also implicitly wrapped in arrays; each job
    in the scatter results in an entry in the output array.

    If `scatter` declares more than one input parameter, he `scatterMethod`
    describes how to decompose the input into a discrete set of jobs.

      * **dotproduct** specifies that each the input arrays are lined up and
          one element taken from each array to construct the job.  It is an
          error if the input arrays are not all the same length.

      * **nested_crossproduct** specifies the cartesian product of the inputs,
          producing a job for every combination of the scattered inputs.  The
          output arrays must be nested for each level of scattering, in the
          order that the input arrays are listed in the `scatter` field.

      * **flat_crossproduct** specifies the cartesian product of the inputs,
          producing a job for every combination of the scattered inputs.  The
          output arrays must be flattened to a single level, but otherwise listed in the
          order that the input arrays are listed in the `scatter` field.

  fields:
    - name: scatter
      type:
        - string
        - type: array
          items: string
      jsonldPredicate:
        "@id": "cwl:scatter"
        "@type": "@id"
        "@container": "@list"
    - name: scatterMethod
      doc: |
        Required if `scatter` is an array of more than one element.
      type:
        - "null"
        - ScatterMethod
      jsonldPredicate:
        "@id": "cwl:scatterMethod"
        "@type": "@vocab"


- type: record
  name: SchemaDef
  extends: Schema
  docParent: SchemaDefRequirement
  fields:
    - name: name
      type: string
      doc: "The type name being defined."


- name: SchemaDefRequirement
  type: record
  extends: ProcessRequirement
  doc: |
        This field consists of an
        array of type definitions which must be used when interpreting the `inputs` and
        `outputs` fields.  When a symbolic type is encountered that is not in
        [`Datatype`](#datatype), the implementation must check if
        the type is defined in `schemaDefs` and use that definition.  If the type is not
        found in `schemaDefs`, it is an error.  The entries in `schemaDefs` must be
        processed in the order listed such that later schema definitions may refer to
        earlier schema definitions.
  fields:
    - name: types
      type:
        type: array
        items: SchemaDef
      doc: The list of type definitions.


- type: record
  name: ExpressionEngineRequirement
  extends: ProcessRequirement
  doc: |
    Define an expression engine, as described in [Expressions](#expressions).

  fields:
    - name: id
      type: string
      doc: "Used to identify the expression engine in the `engine` field of Expressions."
      jsonldPredicate: "@id"
    - name: requirements
      type:
        - "null"
        - type: array
          items: ProcessRequirement
      doc: |
        Requirements to run this expression engine, such as DockerRequirement
        for specifying a container with the engine.
    - name: engineCommand
      type:
        - "null"
        - string
        - type: array
          items: string
      doc: "The command line to invoke the expression engine."
    - name: expressionDefs
      type:
        - "null"
        - type: array
          items: string
      doc: |
        Code fragments that should be evaluated before evaluating the script.
        Can be used to provide function definitions used in multiple places.
